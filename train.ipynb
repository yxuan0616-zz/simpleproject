{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "uHEKSQJaZV86",
        "_4fy7dlbz-cq",
        "W02wH8RQBig_",
        "Hx8wYcL2wZ3L",
        "CSKKpe5arfGP",
        "wMuQPzF6GyvL",
        "Nl1KSS75G3zr",
        "3zehG7tHvdlN",
        "gFh9Sboc7NV0",
        "7IAPXJAUguR7"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yxuan0616/simpleproject/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FBuxK4_4HB1",
        "colab_type": "text"
      },
      "source": [
        "#Training a wave breaker type classifier.\n",
        "input 12 numbers, breaking point and shoreline of three strips. This needs to be changed because Ho and Lo were not put in, so the input data does not include the offshore wave data yet.\n",
        "\n",
        "output: 4 numbers,  probability of 'negative slope',  'Sp', 'Pl', 'Sur'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzXWUs3sYqK7",
        "colab_type": "text"
      },
      "source": [
        "#Make sure the data file is uploaded here everytime running this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRO22ln333Fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle as pk\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import ticker, cm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as utils\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "\n",
        "import os\n",
        "import matplotlib.font_manager\n",
        "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afLUvIGLycdU",
        "colab_type": "text"
      },
      "source": [
        "#testing on 2 days"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV-A-YurryZS",
        "colab_type": "code",
        "outputId": "33c46f6d-8a06-444f-88ac-87113f2e9880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "file = \"partial\"\n",
        "with open(file, \"rb\") as f:\n",
        "    x,poplist,Hos,Los,newIs,Xb,Xs,Xname = pk.load(f)\n",
        "# for index in sorted(poplist, reverse=True):\n",
        "# #    del I[index]\n",
        "#     del Los[index]\n",
        "#     del Hos[index]\n",
        "#     del Y[index]\n",
        "#     del Lab[index]\n",
        "#     del newIs[index]\n",
        "#     del Xname[index]\n",
        "Is = []\n",
        "Ls = []\n",
        "for i in range(len(newIs)):\n",
        "    for j in range(newIs[0].shape[0]):\n",
        "        Is.append(np.concatenate(([Hos[i]],[Los[i]],newIs[i][j]))) ###You need predict this one.\n",
        "\n",
        "# samplefile = \"samples_latest\"\n",
        "# with open(samplefile, \"rb\") as f:\n",
        "#      inputs,targets =pk.load(f)\n",
        "# Spilling=[]\n",
        "# Plunging=[]\n",
        "# Surging=[]\n",
        "# import math\n",
        "# badinput = []\n",
        "# badout = []\n",
        "# for i in range(len(inputs)):\n",
        "#   for j in inputs[i]:\n",
        "#     if math.isinf(j) or math.isnan(j):\n",
        "#       badinput.append(i)\n",
        "#   for p in targets[i]:\n",
        "#     if math.isinf(p) or math.isnan(p):\n",
        "#       badout.append(i)\n",
        "\n",
        "# print(badinput)\n",
        "# print(badout)\n",
        "\n",
        "#save the dirty one incase\n",
        "# dinputs = inputs.copy()\n",
        "# dtargets = targets.copy()\n",
        "\n",
        "#clean the dirty samples\n",
        "# for index in sorted(badout, reverse=True):\n",
        "# #    del I[index]\n",
        "#     del inputs[index]\n",
        "#     del targets[index]\n",
        "######################################put neral network here###########3\n",
        "# regressor1 = RandomForestRegressor(n_estimators=30, random_state=0)\n",
        "# regressor2 = RandomForestRegressor(n_estimators=30, random_state=0)\n",
        "# regressor3 = RandomForestRegressor(n_estimators=30, random_state=0)\n",
        "# regressor1.fit(inputs, Spilling)\n",
        "# regressor2.fit(inputs, Plunging)\n",
        "# regressor3.fit(inputs, Surging)\n",
        "# ySp_RF = np.clip(regressor1.predict(Is),0,1)\n",
        "# yP_RF=np.clip(regressor2.predict(Is),0,1)\n",
        "# ySu_RF=np.clip(regressor3.predict(Is),0,1)\n",
        "# Sum=ySp_RF+yP_RF+ySu_RF\n",
        "# ySp_RF=ySp_RF/Sum\n",
        "# yP_RF=yP_RF/Sum\n",
        "# ySu_RF=ySu_RF/Sum\n",
        "##############################Store Result\n",
        "print(Is[10])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1.05442105 104.02869287 288.71171171 -56.61065266 297.7027027\n",
            " -19.12003001 305.1951952   18.37059265 114.88588589 113.38738739\n",
            " 135.86486486]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgbfddxRvLZP",
        "colab_type": "code",
        "outputId": "d35831da-86ad-48dc-95b2-1ca1f43bc7e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(Is)/4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUiLu7qGtckX",
        "colab_type": "code",
        "outputId": "bd724736-178d-4a64-8bbc-2d5d340972ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#need to upload FC everytime when running here.\n",
        "state_dict = torch.load('FC')\n",
        "hidden1 = 20\n",
        "hidden2 = 40\n",
        "hidden3 = 60\n",
        "inputsize = 11\n",
        "\n",
        "model = nn.Sequential(\n",
        "              nn.Linear(inputsize, hidden1),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden1,hidden2),\n",
        "              nn.ReLU(),\n",
        "#               nn.Linear(hidden2,hidden3),\n",
        "#               nn.ReLU(),\n",
        "          #     nn.Linear(hidden3,hidden4),\n",
        "          #     nn.ReLU(),\n",
        "          #     nn.Linear(hidden4,hidden3),\n",
        "          #     nn.ReLU(),\n",
        "#               nn.Linear(hidden3,hidden2),\n",
        "#               nn.ReLU(),\n",
        "#               nn.Linear(hidden2,hidden1),\n",
        "#               nn.ReLU(),\n",
        "              nn.Linear(hidden2,3),\n",
        "#               nn.Softmax()\n",
        "          ).cuda()\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlCI7b9RuFw6",
        "colab_type": "code",
        "outputId": "4eb226d1-3690-4eb9-ed37-ba3e9efef6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()\n",
        "USE_GPU = True\n",
        "out = []\n",
        "\n",
        "for i in range(len(Is)):\n",
        "  inp = Is[i]\n",
        "\n",
        "  if USE_GPU == True:\n",
        "      inp = torch.Tensor(inp)\n",
        "      inp = inp.cuda()\n",
        "  ou = model(inp)\n",
        "  ou = ou.cpu()\n",
        "\n",
        "  ou = ou.detach().numpy()\n",
        "  out.append(ou)\n",
        "  \n",
        "  \n",
        "print(len(out))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHfcxFUduGRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Lab=[]\n",
        "count=0\n",
        "for i in range(len(Xb)):\n",
        "   temp=[]\n",
        "   for j in range(49):\n",
        "       if USE_GPU == True:\n",
        "        temp.append(out[i])\n",
        "        count+=1\n",
        "   Lab.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3flvCJDcxj7D",
        "colab_type": "code",
        "outputId": "2e005dec-98ea-494e-a6c9-e0c95d58dede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "file = \"Store_NN_test\"\n",
        "with open(file, \"wb\") as f:\n",
        "    pk.dump([Xname,Xb, Xs,Lab], f)\n",
        "print('NN done')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELMt_W2Ptdr8",
        "colab_type": "text"
      },
      "source": [
        "#Below is training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vdtvEIB4F4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pickle load the samples\n",
        "samplefile = \"samples_latest\"\n",
        "with open(samplefile, \"rb\") as f:\n",
        "    inputs,targets = pk.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-Amd2Td5enu",
        "colab_type": "code",
        "outputId": "114ddf3d-5c5b-4473-e916-32087a3d7540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print('inputs samples:',len(inputs))\n",
        "print('input size:',inputs[0].shape)\n",
        "print('target:',len(targets))\n",
        "print('target size:',targets[0].shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs samples: 12201\n",
            "input size: (11,)\n",
            "target: 12201\n",
            "target size: (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23MrBMSAamYy",
        "colab_type": "code",
        "outputId": "30ad0e6b-4ba0-45c2-edb5-bcb382bb1065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(inputs[40][4],inputs[40+45][4],inputs[40+45*2][4])\n",
        "print(targets[40][0],targets[40+45][0],targets[40+45*2][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "245.25525525525526 252.74774774774772 245.25525525525526\n",
            "0.8303 0.8613 0.9519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASs1l3BsZBu8",
        "colab_type": "text"
      },
      "source": [
        "#OMG data cleaning is so important!\n",
        "I almost die tunning a fully connect neural network, since when can't I even train a fully connect neural network??? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCR9hRt4H3XP",
        "colab_type": "code",
        "outputId": "ac948e2f-6779-487d-99ec-cc4f3d4ae90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import math\n",
        "badinput = []\n",
        "badout = []\n",
        "for i in range(len(inputs)):\n",
        "  for j in inputs[i]:\n",
        "    if math.isinf(j) or math.isnan(j):\n",
        "      badinput.append(i)\n",
        "  for p in targets[i]:\n",
        "    if math.isinf(p) or math.isnan(p):\n",
        "      badout.append(i)\n",
        "\n",
        "print(badinput)\n",
        "print(badout)\n",
        "\n",
        "#save the dirty one incase\n",
        "dinputs = inputs.copy()\n",
        "dtargets = targets.copy()\n",
        "\n",
        "#clean the dirty samples\n",
        "for index in sorted(badout, reverse=True):\n",
        "#    del I[index]\n",
        "    del inputs[index]\n",
        "    del targets[index]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[882, 882, 882, 3234, 3234, 3234, 3235, 3235, 3235, 4900, 4900, 4900, 5586, 5586, 5586, 6076, 6076, 6076, 7742, 7742, 7742, 8183, 8183, 8183, 8184, 8184, 8184, 8232, 8232, 8232, 8233, 8233, 8233, 8281, 8281, 8281, 8282, 8282, 8282, 8622, 8622, 8622, 8623, 8623, 8623, 9653, 9653, 9653, 9654, 9654, 9654, 9702, 9702, 9702, 9703, 9703, 9703, 9898, 9898, 9898, 10486, 10486, 10486]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_AWLNfWUL-i",
        "colab_type": "code",
        "outputId": "c095e44a-0382-42a2-da71-fadba570ff5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print('clean inputs samples:',len(inputs))\n",
        "print('input size:',inputs[0].shape)\n",
        "print('clean targets samples:',len(targets))\n",
        "print('target size:',targets[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean inputs samples: 12138\n",
            "input size: (11,)\n",
            "clean targets samples: 12138\n",
            "target size: (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHEKSQJaZV86",
        "colab_type": "text"
      },
      "source": [
        "#Load data into loader to prepare to train.\n",
        "divide them into three sections, train + val + test 70%, 15%, 15%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwRFpwIcNVhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# targets = [np.array([targets[i][0]]) for i in range(len(targets))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqiJr2G_5xwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor_X = torch.stack([torch.Tensor(i) for i in inputs]) # transform to torch tensors\n",
        "tensor_y = torch.stack([torch.Tensor(i) for i in targets])\n",
        "torch.manual_seed(155)\n",
        "shu = torch.randperm(len(tensor_X)) #shuffle\n",
        "tensor_X = tensor_X[shu]\n",
        "tensor_y = tensor_y[shu]\n",
        "\n",
        "train_size = len(inputs)*7//10\n",
        "val_size = len(targets)*85//100\n",
        "X_train = tensor_X[:train_size]\n",
        "y_train = tensor_y[:train_size]\n",
        "X_val = tensor_X[train_size:val_size]\n",
        "y_val = tensor_y[train_size:val_size]\n",
        "X_test = tensor_X[val_size:]\n",
        "y_test = tensor_y[val_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwmaq5yhzoDw",
        "colab_type": "code",
        "outputId": "f03e4d89-f2c1-4993-fd11-4cc7278dcdaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(y_train[100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.9940, 0.0060, 0.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4nFDgGSBGxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=len(X_val),shuffle=True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=len(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCXyQOreBYRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.is_available()\n",
        "USE_GPU = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD9XScoGBaPn",
        "colab_type": "code",
        "outputId": "f05b5c64-b45a-42bd-e4eb-6c241e1563ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "#print_every = 100\n",
        "\n",
        "print('using device:', device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4fy7dlbz-cq",
        "colab_type": "text"
      },
      "source": [
        "#Code from a simple example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7X8nIME0d0j",
        "colab_type": "code",
        "outputId": "de969da3-d5be-4fba-c77e-fd85edb9084b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. Each Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(5):\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Tensor of input data to the Module and it produces\n",
        "    # a Tensor of output data.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "    # values of y, and the loss function returns a Tensor containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    print(t, loss.item())\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 669.1305541992188\n",
            "1 621.9342041015625\n",
            "2 581.0950317382812\n",
            "3 544.9474487304688\n",
            "4 512.69482421875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W02wH8RQBig_",
        "colab_type": "text"
      },
      "source": [
        "#Check accuracy and get number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fwQQSB1BhTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_accuracy(loader, model):\n",
        "#     if loader.dataset.train:\n",
        "#         print('Checking accuracy on validation set')\n",
        "#     else:\n",
        "#         print('Checking accuracy on test set')   \n",
        "\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            output = model(x)\n",
        "            lossf = nn.MSELoss(reduction='mean')\n",
        "            loss = lossf(output,y.float())\n",
        "            print('validation or test loss is',loss)\n",
        "    \n",
        "    return loss\n",
        "  \n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzdtSNmGEzjU",
        "colab_type": "code",
        "outputId": "bcbe0be1-1d1e-40dc-a5bf-11e5a8bd1cf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9_7i5nwXuGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, epochs=1000):\n",
        "    TRAIN_LOSS = []\n",
        "    VAL_LOSS = []\n",
        "    \"\"\"\n",
        "    Train a model to predict breaker type\n",
        "    \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        for t, (x, y) in enumerate(train_loader):\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "            output = model(x)\n",
        "            lossf = nn.MSELoss(reduction='mean')\n",
        "            loss = lossf(output,y.float())\n",
        "            train_loss = loss.data.clone()\n",
        "            TRAIN_LOSS.append(train_loss)\n",
        "            #add reg\n",
        "            reg_loss = 0\n",
        "            for param in model.parameters():\n",
        "                reg_loss += torch.norm(param)\n",
        "            loss += reg * reg_loss\n",
        "            \n",
        "            \n",
        "            # Zero out all of the gradients for the variables which the optimizer\n",
        "            # will update.\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # This is the backwards pass: compute the gradient of the loss with\n",
        "            # respect to each  parameter of the model.\n",
        "            loss.backward()\n",
        "\n",
        "            # Actually update the parameters of the model using the gradients\n",
        "            # computed by the backwards pass.\n",
        "            optimizer.step()\n",
        "\n",
        "            if t % print_every == 0:\n",
        "#                 print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
        "                val_loss = check_accuracy(val_loader, model)\n",
        "                VAL_LOSS.append(val_loss)\n",
        "    \n",
        "    return TRAIN_LOSS, VAL_LOSS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx8wYcL2wZ3L",
        "colab_type": "text"
      },
      "source": [
        "#Hyper Parameter space swiping\n",
        "Search for the best set of neurons numbers in each layer, the learning rate, the regurilaztion rate. Save the best validation and model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfZx9JO2XziS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hidden1 = 20\n",
        "# hidden2 = 40\n",
        "# hidden3 = 60\n",
        "# hidden4 = 100\n",
        "H = [20,40,60,80]\n",
        "print_every = 10000\n",
        "learning_rates = [1e-3,3e-3,7e-3,1e-2]\n",
        "regs = [1e-4, 3e-4, 7e-4, 1e-3]\n",
        "inputsize = inputs[0].shape[0]\n",
        "models = []\n",
        "optimizer = []\n",
        "best_model = None\n",
        "best_val = 100\n",
        "results = {}\n",
        "def w_init(m):\n",
        "    if type(m) ==  nn.Conv2d or type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight.data)\n",
        "        #random_weight did not work well.\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "  for hidden1 in H:\n",
        "    for hidden2 in H:\n",
        "      for hidden3 in H:\n",
        "        for reg in regs:\n",
        "          model = nn.Sequential(\n",
        "              nn.Linear(inputsize, hidden1),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden1,hidden2),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden2,hidden3),\n",
        "              nn.ReLU(),\n",
        "          #     nn.Linear(hidden3,hidden4),\n",
        "          #     nn.ReLU(),\n",
        "          #     nn.Linear(hidden4,hidden3),\n",
        "          #     nn.ReLU(),\n",
        "              nn.Linear(hidden3,hidden2),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden2,hidden1),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden1,3),\n",
        "              nn.Softmax()\n",
        "          ).to(device)\n",
        "          optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "          # optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
        "          # optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0.1,weight_decay=reg, initial_accumulator_value=0)\n",
        "          start =time.time()\n",
        "          model.apply(w_init)\n",
        "          TRAIN_LOSS, VAL_LOSS = train(model, optimizer)\n",
        "          \n",
        "          end = time.time()\n",
        "          traintime = end - start\n",
        "          print('Training time with gpu is', traintime)\n",
        "\n",
        "          VAL_LOSS = [VAL_LOSS[i].cpu() for i in range(len(VAL_LOSS))]\n",
        "          VAL_LOSS = [VAL_LOSS[I].detach().numpy() for I in range(len(VAL_LOSS))]\n",
        "          TRAIN_LOSS = [TRAIN_LOSS[i].cpu() for i in range(len(VAL_LOSS))]\n",
        "          TRAIN_LOSS = [TRAIN_LOSS[I].detach().numpy() for I in range(len(TRAIN_LOSS))]\n",
        "          results[learning_rate, hidden1,hidden2,hidden3,reg] = (model,TRAIN_LOSS,VAL_LOSS)\n",
        "#           caches.append(cache)\n",
        "          if VAL_LOSS[-1] < best_val:\n",
        "            best_val = VAL_LOSS[-1]   \n",
        "            best_model = model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXovAQCIcB67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_val\n",
        "best_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4VWWKBYcQLe",
        "colab_type": "code",
        "outputId": "44bfe1dd-eb44-4800-8ceb-2ec05ea994f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "for learning_rate in learning_rates:\n",
        "  for hidden1 in H:\n",
        "    for hidden2 in H:\n",
        "      for hidden3 in H:\n",
        "        for reg in regs:\n",
        "          model,TRAIN_LOSS,VAL_LOSS =  results[learning_rate, hidden1,hidden2,hidden3,reg]\n",
        "          if VAL_LOSS[-1] < 0.13 and TRAIN_LOSS[-1] < 0.13:\n",
        "            print(learning_rate, hidden1,hidden2,hidden3,reg)\n",
        "            print(VAL_LOSS[-1],TRAIN_LOSS[-1])\n",
        "            #0.001 20 80 40 0.001\n",
        "            plt.plot(VAL_LOSS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-196-163eef5a487a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mhidden3\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTRAIN_LOSS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVAL_LOSS\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mVAL_LOSS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.13\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mTRAIN_LOSS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.13\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (0.001, 20, 20, 20, 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhHLsIRGeuSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_model,TRAIN_LOSS,VAL_LOSS = results[0.001,20,80,40,0.001]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuhFa6gRs1-z",
        "colab_type": "code",
        "outputId": "8151151d-0aef-46fc-9e50-43d240138a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.6870083510875702"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5VcPggCs-YB",
        "colab_type": "code",
        "outputId": "e29a3024-7738-46fb-be9d-f54d91ff1722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "source": [
        "print(VAL_LOSS)\n",
        "TRAIN_LOSS\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "t = range(100)\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(t, TRAIN_LOSS)\n",
        "plt.plot(t, VAL_LOSS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array(0.31067604, dtype=float32), array(0.14452095, dtype=float32), array(0.12789679, dtype=float32), array(0.12261596, dtype=float32), array(0.12074573, dtype=float32), array(0.11333477, dtype=float32), array(0.11237621, dtype=float32), array(0.11172884, dtype=float32), array(0.11187048, dtype=float32), array(0.11172836, dtype=float32), array(0.11192539, dtype=float32), array(0.11152691, dtype=float32), array(0.11161321, dtype=float32), array(0.11154135, dtype=float32), array(0.11147907, dtype=float32), array(0.11203841, dtype=float32), array(0.11149548, dtype=float32), array(0.11153662, dtype=float32), array(0.11141267, dtype=float32), array(0.11140399, dtype=float32), array(0.1114194, dtype=float32), array(0.11148959, dtype=float32), array(0.11140756, dtype=float32), array(0.11137823, dtype=float32), array(0.11141394, dtype=float32), array(0.11150812, dtype=float32), array(0.11181144, dtype=float32), array(0.11179108, dtype=float32), array(0.1118466, dtype=float32), array(0.11135422, dtype=float32), array(0.11133776, dtype=float32), array(0.11133714, dtype=float32), array(0.11174813, dtype=float32), array(0.11155719, dtype=float32), array(0.1113621, dtype=float32), array(0.11133628, dtype=float32), array(0.11133352, dtype=float32), array(0.11139292, dtype=float32), array(0.11131715, dtype=float32), array(0.11131878, dtype=float32), array(0.11130703, dtype=float32), array(0.1113264, dtype=float32), array(0.1113245, dtype=float32), array(0.1116674, dtype=float32), array(0.11135036, dtype=float32), array(0.11129805, dtype=float32), array(0.11129723, dtype=float32), array(0.11132923, dtype=float32), array(0.11129498, dtype=float32), array(0.11133911, dtype=float32), array(0.11129303, dtype=float32), array(0.11130055, dtype=float32), array(0.11129188, dtype=float32), array(0.11132234, dtype=float32), array(0.11129393, dtype=float32), array(0.11130349, dtype=float32), array(0.11129414, dtype=float32), array(0.11131385, dtype=float32), array(0.11134195, dtype=float32), array(0.11128847, dtype=float32), array(0.11132819, dtype=float32), array(0.11129498, dtype=float32), array(0.1113996, dtype=float32), array(0.11128026, dtype=float32), array(0.11128415, dtype=float32), array(0.11128719, dtype=float32), array(0.11127798, dtype=float32), array(0.11132893, dtype=float32), array(0.11128426, dtype=float32), array(0.11128824, dtype=float32), array(0.1113099, dtype=float32), array(0.11130365, dtype=float32), array(0.11128116, dtype=float32), array(0.11127391, dtype=float32), array(0.11128203, dtype=float32), array(0.11132707, dtype=float32), array(0.11128833, dtype=float32), array(0.11136577, dtype=float32), array(0.11128118, dtype=float32), array(0.11127418, dtype=float32), array(0.11130732, dtype=float32), array(0.11127964, dtype=float32), array(0.11128682, dtype=float32), array(0.11130647, dtype=float32), array(0.11127047, dtype=float32), array(0.11138089, dtype=float32), array(0.11127134, dtype=float32), array(0.11127386, dtype=float32), array(0.11127069, dtype=float32), array(0.11128534, dtype=float32), array(0.11126804, dtype=float32), array(0.11127023, dtype=float32), array(0.11127501, dtype=float32), array(0.11128431, dtype=float32), array(0.11128885, dtype=float32), array(0.11126588, dtype=float32), array(0.11126691, dtype=float32), array(0.11126871, dtype=float32), array(0.11127263, dtype=float32), array(0.11126421, dtype=float32)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc9270182e8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2047\u001b[0m                         \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2048\u001b[0m                         \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2049\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   2050\u001b[0m                     \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \"\"\"\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mtoolbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1649\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2626\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2628\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[0;32m-> 1187\u001b[0;31m                                                                 renderer)\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks_to_draw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1On\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m                 \u001b[0mextent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_window_extent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m                 \u001b[0mticklabelBoxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel2On\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get window extent w/o renderer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 w, h, d = renderer.get_text_width_height_descent(clean_line,\n\u001b[1;32m    312\u001b[0m                                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                                         ismath=ismath)\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_in_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             w, h, d = texmanager.get_text_width_height_descent(\n\u001b[0;32m--> 209\u001b[0;31m                 s, fontsize, renderer=self)\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# use dviread. It sometimes returns a wrong descent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0mdvifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdviread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdpi_fraction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mmake_dvi\u001b[0;34m(self, tex, fontsize)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 self._run_checked_subprocess(\n\u001b[1;32m    327\u001b[0m                     [\"latex\", \"-interaction=nonstopmode\", \"--halt-on-error\",\n\u001b[0;32m--> 328\u001b[0;31m                      texfile], tex)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasefile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dvi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex)\u001b[0m\n\u001b[1;32m    296\u001b[0m             report = subprocess.check_output(command,\n\u001b[1;32m    297\u001b[0m                                              \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                                              stderr=subprocess.STDOUT)\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 356\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stdin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    727\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1362\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'latex': 'latex'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53Gx5CaiqfOf",
        "colab_type": "code",
        "outputId": "b7c2236c-237d-4b25-eaa9-85ba015e6506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "# from matplotlib import rc\n",
        "# import matplotlib.pyplot as plt\n",
        "# t = range(100)\n",
        "# fig, ax = plt.subplots()\n",
        "# red dashes, blue squares and green triangles\n",
        "# ax.plot(t, TRAIN_LOSS, 'r-', label = 'Train loss', linewidth=7.0)\n",
        "# ax.plot(t, VAL_LOSS, 'y-.',label = 'Validation loss', linewidth=7.0)\n",
        "plt.plot(TRAIN_LOSS)\n",
        "# #    plt.gcf().autofmt_xdate()\n",
        "# leg = ax.legend()\n",
        "# leg.prop={'size':14}\n",
        "# #    plt.show()\n",
        "# plt.legend(fontsize='40', title_fontsize='80')\n",
        "\n",
        "# plt.xlabel('Epoch',fontsize = 60)\n",
        "# plt.ylabel('MSE Loss',fontsize = 60)\n",
        "# plt.rc('text', usetex=True)\n",
        "# plt.rc('font', family='serif')\n",
        "# plt.xticks(size = 60)\n",
        "# plt.yticks(size = 60)\n",
        "# ax.set_xlim([0,24.5])\n",
        "# beautify the x-labels\n",
        "#    plt.gcf().autofmt_xdate()\n",
        "# plt.title('Training and validation loss',fontsize = 80)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc926256ac8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2047\u001b[0m                         \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2048\u001b[0m                         \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2049\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   2050\u001b[0m                     \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \"\"\"\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mtoolbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1649\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2626\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2628\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[0;32m-> 1187\u001b[0;31m                                                                 renderer)\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks_to_draw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1On\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m                 \u001b[0mextent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_window_extent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m                 \u001b[0mticklabelBoxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel2On\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get window extent w/o renderer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 w, h, d = renderer.get_text_width_height_descent(clean_line,\n\u001b[1;32m    312\u001b[0m                                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                                         ismath=ismath)\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_in_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             w, h, d = texmanager.get_text_width_height_descent(\n\u001b[0;32m--> 209\u001b[0;31m                 s, fontsize, renderer=self)\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# use dviread. It sometimes returns a wrong descent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0mdvifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdviread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdpi_fraction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mmake_dvi\u001b[0;34m(self, tex, fontsize)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 self._run_checked_subprocess(\n\u001b[1;32m    327\u001b[0m                     [\"latex\", \"-interaction=nonstopmode\", \"--halt-on-error\",\n\u001b[0;32m--> 328\u001b[0;31m                      texfile], tex)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasefile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dvi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex)\u001b[0m\n\u001b[1;32m    296\u001b[0m             report = subprocess.check_output(command,\n\u001b[1;32m    297\u001b[0m                                              \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                                              stderr=subprocess.STDOUT)\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 356\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stdin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    727\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1362\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'latex': 'latex'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSKKpe5arfGP",
        "colab_type": "text"
      },
      "source": [
        "#Coarse hyper parameter space swiping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZDffF00kFDO",
        "colab_type": "code",
        "outputId": "cd08c562-0f2b-4eb1-94a2-4d28e221aaf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# hidden1 = 20\n",
        "# hidden2 = 40\n",
        "# hidden3 = 60\n",
        "# hidden4 = 100\n",
        "H = [20,50,100]\n",
        "print_every = 10000\n",
        "learning_rates = [1e-3]#1e-4,5e-4,5e-3,1e-2]\n",
        "regs = [0]\n",
        "inputsize = inputs[0].shape[0]\n",
        "models = []\n",
        "optimizer = []\n",
        "coarsebest_model = None\n",
        "coarsebest_val = 100\n",
        "coarseresults = {}\n",
        "def w_init(m):\n",
        "    if type(m) ==  nn.Conv2d or type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight.data)\n",
        "        #random_weight did not work well.\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "  for hidden1 in H:\n",
        "    for hidden2 in H:\n",
        "      for hidden3 in H:\n",
        "        for reg in regs:\n",
        "          model = nn.Sequential(\n",
        "              nn.Linear(inputsize, hidden1),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden1,hidden2),\n",
        "              nn.ReLU(),\n",
        "#               nn.Linear(hidden2,hidden3),\n",
        "#               nn.ReLU(),\n",
        "          #     nn.Linear(hidden3,hidden4),\n",
        "          #     nn.ReLU(),\n",
        "          #     nn.Linear(hidden4,hidden3),\n",
        "          #     nn.ReLU(),\n",
        "#               nn.Linear(hidden3,hidden2),\n",
        "#               nn.ReLU(),\n",
        "              nn.Linear(hidden2,hidden1),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden1,3),\n",
        "#               nn.Softmax()\n",
        "          ).to(device)\n",
        "          optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "          # optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
        "          # optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0.1,weight_decay=reg, initial_accumulator_value=0)\n",
        "          start =time.time()\n",
        "          model.apply(w_init)\n",
        "          TRAIN_LOSS, VAL_LOSS = train(model, optimizer)\n",
        "          \n",
        "          end = time.time()\n",
        "          traintime = end - start\n",
        "          print('Training time with gpu is', traintime)\n",
        "\n",
        "          VAL_LOSS = [VAL_LOSS[i].cpu() for i in range(len(VAL_LOSS))]\n",
        "          VAL_LOSS = [VAL_LOSS[I].detach().numpy() for I in range(len(VAL_LOSS))]\n",
        "          TRAIN_LOSS = [TRAIN_LOSS[i].cpu() for i in range(len(VAL_LOSS))]\n",
        "          TRAIN_LOSS = [TRAIN_LOSS[I].detach().numpy() for I in range(len(TRAIN_LOSS))]\n",
        "          coarseresults[learning_rate, hidden1,hidden2,hidden3,reg] = (model,TRAIN_LOSS,VAL_LOSS)\n",
        "#           caches.append(cache)\n",
        "          if VAL_LOSS[-1] < coarsebest_val:\n",
        "            coarsebest_val = VAL_LOSS[-1]   \n",
        "            coarsebest_model = model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation or test loss is tensor(2.4297e+11, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n",
            "validation or test loss is tensor(nan, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-200-a291b6d8392b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m           \u001b[0mTRAIN_LOSS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_LOSS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-41a40fda203e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# move the model parameters to CPU/GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# put model to training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# move to device, e.g. GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPmQv0JsvVH8",
        "colab_type": "text"
      },
      "source": [
        "#Trivial training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ7Xx7GQbYkZ",
        "colab_type": "code",
        "outputId": "bb09f4e9-5874-49c5-da56-7ba7336aa194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hidden1 = 20\n",
        "hidden2 = 40\n",
        "hidden3 = 60\n",
        "hidden4 = 100\n",
        "\n",
        "print_every = 1000\n",
        "learning_rate = 1e-2\n",
        "reg = 1e-3\n",
        "inputsize = inputs[0].shape[0]\n",
        "print(inputsize)\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "def w_init(m):\n",
        "    if type(m) ==  nn.Conv2d or type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight.data)\n",
        "        #random_weight did not work well.\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "        \n",
        "model = nn.Sequential(\n",
        "    nn.Linear(inputsize, hidden1),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden1,hidden2),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden2,3),\n",
        "  #  nn.ReLU(),\n",
        "#    nn.Linear(hidden3,hidden4),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(hidden4,hidden3),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(hidden3,hidden2),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(hidden2,hidden1),\n",
        "    #nn.ReLU(),\n",
        "    #nn.Linear(hidden3,3),\n",
        "    #nn.ReLU(),\n",
        "    #nn.Softmax(),\n",
        ").to(device)\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
        "# optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0.1,weight_decay=reg, initial_accumulator_value=0)\n",
        "start =time.time()\n",
        "model.apply(w_init)\n",
        "TRAIN_LOSS, VAL_LOSS = train(model, optimizer)\n",
        "end = time.time()\n",
        "traintime = end - start\n",
        "print('Training time with gpu is', traintime)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n",
            "validation or test loss is tensor(1605.2352, device='cuda:0')\n",
            "validation or test loss is tensor(80.7479, device='cuda:0')\n",
            "validation or test loss is tensor(37.9590, device='cuda:0')\n",
            "validation or test loss is tensor(22.6352, device='cuda:0')\n",
            "validation or test loss is tensor(16.8442, device='cuda:0')\n",
            "validation or test loss is tensor(13.2792, device='cuda:0')\n",
            "validation or test loss is tensor(9.8962, device='cuda:0')\n",
            "validation or test loss is tensor(7.7936, device='cuda:0')\n",
            "validation or test loss is tensor(6.1726, device='cuda:0')\n",
            "validation or test loss is tensor(4.5014, device='cuda:0')\n",
            "validation or test loss is tensor(3.6798, device='cuda:0')\n",
            "validation or test loss is tensor(3.1151, device='cuda:0')\n",
            "validation or test loss is tensor(2.7861, device='cuda:0')\n",
            "validation or test loss is tensor(2.5802, device='cuda:0')\n",
            "validation or test loss is tensor(2.1455, device='cuda:0')\n",
            "validation or test loss is tensor(1.9620, device='cuda:0')\n",
            "validation or test loss is tensor(1.8133, device='cuda:0')\n",
            "validation or test loss is tensor(1.6686, device='cuda:0')\n",
            "validation or test loss is tensor(1.5334, device='cuda:0')\n",
            "validation or test loss is tensor(1.4129, device='cuda:0')\n",
            "validation or test loss is tensor(1.3247, device='cuda:0')\n",
            "validation or test loss is tensor(1.2412, device='cuda:0')\n",
            "validation or test loss is tensor(1.1727, device='cuda:0')\n",
            "validation or test loss is tensor(1.0702, device='cuda:0')\n",
            "validation or test loss is tensor(1.0354, device='cuda:0')\n",
            "validation or test loss is tensor(0.9648, device='cuda:0')\n",
            "validation or test loss is tensor(0.9403, device='cuda:0')\n",
            "validation or test loss is tensor(0.8569, device='cuda:0')\n",
            "validation or test loss is tensor(0.8834, device='cuda:0')\n",
            "validation or test loss is tensor(0.7871, device='cuda:0')\n",
            "validation or test loss is tensor(0.7745, device='cuda:0')\n",
            "validation or test loss is tensor(0.7666, device='cuda:0')\n",
            "validation or test loss is tensor(0.6709, device='cuda:0')\n",
            "validation or test loss is tensor(0.6752, device='cuda:0')\n",
            "validation or test loss is tensor(0.6318, device='cuda:0')\n",
            "validation or test loss is tensor(0.5988, device='cuda:0')\n",
            "validation or test loss is tensor(0.5664, device='cuda:0')\n",
            "validation or test loss is tensor(0.5424, device='cuda:0')\n",
            "validation or test loss is tensor(0.5810, device='cuda:0')\n",
            "validation or test loss is tensor(0.5176, device='cuda:0')\n",
            "validation or test loss is tensor(0.4928, device='cuda:0')\n",
            "validation or test loss is tensor(0.5009, device='cuda:0')\n",
            "validation or test loss is tensor(0.5075, device='cuda:0')\n",
            "validation or test loss is tensor(0.4428, device='cuda:0')\n",
            "validation or test loss is tensor(0.4396, device='cuda:0')\n",
            "validation or test loss is tensor(0.4279, device='cuda:0')\n",
            "validation or test loss is tensor(0.4278, device='cuda:0')\n",
            "validation or test loss is tensor(0.3979, device='cuda:0')\n",
            "validation or test loss is tensor(0.3983, device='cuda:0')\n",
            "validation or test loss is tensor(0.3870, device='cuda:0')\n",
            "validation or test loss is tensor(0.4059, device='cuda:0')\n",
            "validation or test loss is tensor(0.3481, device='cuda:0')\n",
            "validation or test loss is tensor(0.3972, device='cuda:0')\n",
            "validation or test loss is tensor(0.3191, device='cuda:0')\n",
            "validation or test loss is tensor(0.3160, device='cuda:0')\n",
            "validation or test loss is tensor(0.3221, device='cuda:0')\n",
            "validation or test loss is tensor(0.3323, device='cuda:0')\n",
            "validation or test loss is tensor(0.2925, device='cuda:0')\n",
            "validation or test loss is tensor(0.2912, device='cuda:0')\n",
            "validation or test loss is tensor(0.3019, device='cuda:0')\n",
            "validation or test loss is tensor(0.2792, device='cuda:0')\n",
            "validation or test loss is tensor(0.2832, device='cuda:0')\n",
            "validation or test loss is tensor(0.2699, device='cuda:0')\n",
            "validation or test loss is tensor(0.2581, device='cuda:0')\n",
            "validation or test loss is tensor(0.2452, device='cuda:0')\n",
            "validation or test loss is tensor(0.2389, device='cuda:0')\n",
            "validation or test loss is tensor(0.2986, device='cuda:0')\n",
            "validation or test loss is tensor(0.2380, device='cuda:0')\n",
            "validation or test loss is tensor(0.2186, device='cuda:0')\n",
            "validation or test loss is tensor(0.2171, device='cuda:0')\n",
            "validation or test loss is tensor(0.2226, device='cuda:0')\n",
            "validation or test loss is tensor(0.2028, device='cuda:0')\n",
            "validation or test loss is tensor(0.1973, device='cuda:0')\n",
            "validation or test loss is tensor(0.1921, device='cuda:0')\n",
            "validation or test loss is tensor(0.2343, device='cuda:0')\n",
            "validation or test loss is tensor(0.1922, device='cuda:0')\n",
            "validation or test loss is tensor(0.1899, device='cuda:0')\n",
            "validation or test loss is tensor(0.1820, device='cuda:0')\n",
            "validation or test loss is tensor(0.1965, device='cuda:0')\n",
            "validation or test loss is tensor(0.1669, device='cuda:0')\n",
            "validation or test loss is tensor(0.1679, device='cuda:0')\n",
            "validation or test loss is tensor(0.1682, device='cuda:0')\n",
            "validation or test loss is tensor(0.1579, device='cuda:0')\n",
            "validation or test loss is tensor(0.1662, device='cuda:0')\n",
            "validation or test loss is tensor(0.1594, device='cuda:0')\n",
            "validation or test loss is tensor(0.1772, device='cuda:0')\n",
            "validation or test loss is tensor(0.1814, device='cuda:0')\n",
            "validation or test loss is tensor(0.1491, device='cuda:0')\n",
            "validation or test loss is tensor(0.1397, device='cuda:0')\n",
            "validation or test loss is tensor(0.1419, device='cuda:0')\n",
            "validation or test loss is tensor(0.2087, device='cuda:0')\n",
            "validation or test loss is tensor(0.1340, device='cuda:0')\n",
            "validation or test loss is tensor(0.1622, device='cuda:0')\n",
            "validation or test loss is tensor(0.1288, device='cuda:0')\n",
            "validation or test loss is tensor(0.1397, device='cuda:0')\n",
            "validation or test loss is tensor(0.1577, device='cuda:0')\n",
            "validation or test loss is tensor(0.1197, device='cuda:0')\n",
            "validation or test loss is tensor(0.1185, device='cuda:0')\n",
            "validation or test loss is tensor(0.1189, device='cuda:0')\n",
            "validation or test loss is tensor(0.1163, device='cuda:0')\n",
            "validation or test loss is tensor(0.1362, device='cuda:0')\n",
            "validation or test loss is tensor(0.1654, device='cuda:0')\n",
            "validation or test loss is tensor(0.1822, device='cuda:0')\n",
            "validation or test loss is tensor(0.1164, device='cuda:0')\n",
            "validation or test loss is tensor(0.1064, device='cuda:0')\n",
            "validation or test loss is tensor(0.1079, device='cuda:0')\n",
            "validation or test loss is tensor(0.1017, device='cuda:0')\n",
            "validation or test loss is tensor(0.1871, device='cuda:0')\n",
            "validation or test loss is tensor(0.0944, device='cuda:0')\n",
            "validation or test loss is tensor(0.0932, device='cuda:0')\n",
            "validation or test loss is tensor(0.1018, device='cuda:0')\n",
            "validation or test loss is tensor(0.1236, device='cuda:0')\n",
            "validation or test loss is tensor(0.1337, device='cuda:0')\n",
            "validation or test loss is tensor(0.0850, device='cuda:0')\n",
            "validation or test loss is tensor(0.0920, device='cuda:0')\n",
            "validation or test loss is tensor(0.1027, device='cuda:0')\n",
            "validation or test loss is tensor(0.1412, device='cuda:0')\n",
            "validation or test loss is tensor(0.1914, device='cuda:0')\n",
            "validation or test loss is tensor(0.0798, device='cuda:0')\n",
            "validation or test loss is tensor(0.1087, device='cuda:0')\n",
            "validation or test loss is tensor(0.0809, device='cuda:0')\n",
            "validation or test loss is tensor(0.0747, device='cuda:0')\n",
            "validation or test loss is tensor(0.0836, device='cuda:0')\n",
            "validation or test loss is tensor(0.0710, device='cuda:0')\n",
            "validation or test loss is tensor(0.0758, device='cuda:0')\n",
            "validation or test loss is tensor(0.0986, device='cuda:0')\n",
            "validation or test loss is tensor(0.0663, device='cuda:0')\n",
            "validation or test loss is tensor(0.0667, device='cuda:0')\n",
            "validation or test loss is tensor(0.1804, device='cuda:0')\n",
            "validation or test loss is tensor(0.1302, device='cuda:0')\n",
            "validation or test loss is tensor(0.0685, device='cuda:0')\n",
            "validation or test loss is tensor(0.0663, device='cuda:0')\n",
            "validation or test loss is tensor(0.0777, device='cuda:0')\n",
            "validation or test loss is tensor(0.1995, device='cuda:0')\n",
            "validation or test loss is tensor(0.1394, device='cuda:0')\n",
            "validation or test loss is tensor(0.0972, device='cuda:0')\n",
            "validation or test loss is tensor(0.4485, device='cuda:0')\n",
            "validation or test loss is tensor(0.0668, device='cuda:0')\n",
            "validation or test loss is tensor(0.0755, device='cuda:0')\n",
            "validation or test loss is tensor(0.0695, device='cuda:0')\n",
            "validation or test loss is tensor(0.0731, device='cuda:0')\n",
            "validation or test loss is tensor(0.0573, device='cuda:0')\n",
            "validation or test loss is tensor(0.0570, device='cuda:0')\n",
            "validation or test loss is tensor(0.0708, device='cuda:0')\n",
            "validation or test loss is tensor(0.1013, device='cuda:0')\n",
            "validation or test loss is tensor(0.1052, device='cuda:0')\n",
            "validation or test loss is tensor(0.1619, device='cuda:0')\n",
            "validation or test loss is tensor(0.2148, device='cuda:0')\n",
            "validation or test loss is tensor(0.2290, device='cuda:0')\n",
            "validation or test loss is tensor(0.0571, device='cuda:0')\n",
            "validation or test loss is tensor(0.0559, device='cuda:0')\n",
            "validation or test loss is tensor(0.0530, device='cuda:0')\n",
            "validation or test loss is tensor(0.0494, device='cuda:0')\n",
            "validation or test loss is tensor(0.0467, device='cuda:0')\n",
            "validation or test loss is tensor(0.0565, device='cuda:0')\n",
            "validation or test loss is tensor(0.0573, device='cuda:0')\n",
            "validation or test loss is tensor(0.0621, device='cuda:0')\n",
            "validation or test loss is tensor(0.0547, device='cuda:0')\n",
            "validation or test loss is tensor(0.0814, device='cuda:0')\n",
            "validation or test loss is tensor(0.1506, device='cuda:0')\n",
            "validation or test loss is tensor(0.1203, device='cuda:0')\n",
            "validation or test loss is tensor(0.0815, device='cuda:0')\n",
            "validation or test loss is tensor(0.0581, device='cuda:0')\n",
            "validation or test loss is tensor(0.0644, device='cuda:0')\n",
            "validation or test loss is tensor(0.0380, device='cuda:0')\n",
            "validation or test loss is tensor(0.0636, device='cuda:0')\n",
            "validation or test loss is tensor(0.0504, device='cuda:0')\n",
            "validation or test loss is tensor(0.0406, device='cuda:0')\n",
            "validation or test loss is tensor(0.0503, device='cuda:0')\n",
            "validation or test loss is tensor(0.0389, device='cuda:0')\n",
            "validation or test loss is tensor(0.0717, device='cuda:0')\n",
            "validation or test loss is tensor(0.5233, device='cuda:0')\n",
            "validation or test loss is tensor(0.0927, device='cuda:0')\n",
            "validation or test loss is tensor(0.0456, device='cuda:0')\n",
            "validation or test loss is tensor(0.1091, device='cuda:0')\n",
            "validation or test loss is tensor(0.0566, device='cuda:0')\n",
            "validation or test loss is tensor(0.0603, device='cuda:0')\n",
            "validation or test loss is tensor(0.0422, device='cuda:0')\n",
            "validation or test loss is tensor(0.0762, device='cuda:0')\n",
            "validation or test loss is tensor(0.0381, device='cuda:0')\n",
            "validation or test loss is tensor(0.1053, device='cuda:0')\n",
            "validation or test loss is tensor(0.1036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0407, device='cuda:0')\n",
            "validation or test loss is tensor(0.0427, device='cuda:0')\n",
            "validation or test loss is tensor(0.0313, device='cuda:0')\n",
            "validation or test loss is tensor(0.0343, device='cuda:0')\n",
            "validation or test loss is tensor(0.0633, device='cuda:0')\n",
            "validation or test loss is tensor(0.0393, device='cuda:0')\n",
            "validation or test loss is tensor(0.0322, device='cuda:0')\n",
            "validation or test loss is tensor(0.0367, device='cuda:0')\n",
            "validation or test loss is tensor(0.0557, device='cuda:0')\n",
            "validation or test loss is tensor(0.0279, device='cuda:0')\n",
            "validation or test loss is tensor(0.1204, device='cuda:0')\n",
            "validation or test loss is tensor(0.0599, device='cuda:0')\n",
            "validation or test loss is tensor(0.0257, device='cuda:0')\n",
            "validation or test loss is tensor(0.0252, device='cuda:0')\n",
            "validation or test loss is tensor(0.0309, device='cuda:0')\n",
            "validation or test loss is tensor(0.1801, device='cuda:0')\n",
            "validation or test loss is tensor(0.0430, device='cuda:0')\n",
            "validation or test loss is tensor(0.0414, device='cuda:0')\n",
            "validation or test loss is tensor(0.0466, device='cuda:0')\n",
            "validation or test loss is tensor(0.0814, device='cuda:0')\n",
            "validation or test loss is tensor(0.0855, device='cuda:0')\n",
            "validation or test loss is tensor(0.0355, device='cuda:0')\n",
            "validation or test loss is tensor(0.0364, device='cuda:0')\n",
            "validation or test loss is tensor(0.0349, device='cuda:0')\n",
            "validation or test loss is tensor(0.0324, device='cuda:0')\n",
            "validation or test loss is tensor(0.0311, device='cuda:0')\n",
            "validation or test loss is tensor(0.0859, device='cuda:0')\n",
            "validation or test loss is tensor(0.1108, device='cuda:0')\n",
            "validation or test loss is tensor(0.0443, device='cuda:0')\n",
            "validation or test loss is tensor(0.0280, device='cuda:0')\n",
            "validation or test loss is tensor(0.0905, device='cuda:0')\n",
            "validation or test loss is tensor(0.2870, device='cuda:0')\n",
            "validation or test loss is tensor(0.0970, device='cuda:0')\n",
            "validation or test loss is tensor(0.0316, device='cuda:0')\n",
            "validation or test loss is tensor(0.0218, device='cuda:0')\n",
            "validation or test loss is tensor(0.0214, device='cuda:0')\n",
            "validation or test loss is tensor(0.0311, device='cuda:0')\n",
            "validation or test loss is tensor(0.0596, device='cuda:0')\n",
            "validation or test loss is tensor(0.0318, device='cuda:0')\n",
            "validation or test loss is tensor(0.0277, device='cuda:0')\n",
            "validation or test loss is tensor(0.0233, device='cuda:0')\n",
            "validation or test loss is tensor(0.0651, device='cuda:0')\n",
            "validation or test loss is tensor(0.0304, device='cuda:0')\n",
            "validation or test loss is tensor(0.0433, device='cuda:0')\n",
            "validation or test loss is tensor(0.1718, device='cuda:0')\n",
            "validation or test loss is tensor(0.3432, device='cuda:0')\n",
            "validation or test loss is tensor(0.0722, device='cuda:0')\n",
            "validation or test loss is tensor(0.0229, device='cuda:0')\n",
            "validation or test loss is tensor(0.0238, device='cuda:0')\n",
            "validation or test loss is tensor(0.0298, device='cuda:0')\n",
            "validation or test loss is tensor(0.0183, device='cuda:0')\n",
            "validation or test loss is tensor(0.0207, device='cuda:0')\n",
            "validation or test loss is tensor(0.0175, device='cuda:0')\n",
            "validation or test loss is tensor(0.0206, device='cuda:0')\n",
            "validation or test loss is tensor(0.0235, device='cuda:0')\n",
            "validation or test loss is tensor(0.0377, device='cuda:0')\n",
            "validation or test loss is tensor(0.0515, device='cuda:0')\n",
            "validation or test loss is tensor(0.0206, device='cuda:0')\n",
            "validation or test loss is tensor(0.0226, device='cuda:0')\n",
            "validation or test loss is tensor(0.0739, device='cuda:0')\n",
            "validation or test loss is tensor(0.0287, device='cuda:0')\n",
            "validation or test loss is tensor(0.0183, device='cuda:0')\n",
            "validation or test loss is tensor(0.0246, device='cuda:0')\n",
            "validation or test loss is tensor(0.1131, device='cuda:0')\n",
            "validation or test loss is tensor(0.0627, device='cuda:0')\n",
            "validation or test loss is tensor(0.0234, device='cuda:0')\n",
            "validation or test loss is tensor(0.0229, device='cuda:0')\n",
            "validation or test loss is tensor(0.0167, device='cuda:0')\n",
            "validation or test loss is tensor(0.0191, device='cuda:0')\n",
            "validation or test loss is tensor(0.0274, device='cuda:0')\n",
            "validation or test loss is tensor(0.0159, device='cuda:0')\n",
            "validation or test loss is tensor(0.0163, device='cuda:0')\n",
            "validation or test loss is tensor(0.1406, device='cuda:0')\n",
            "validation or test loss is tensor(0.1278, device='cuda:0')\n",
            "validation or test loss is tensor(0.0440, device='cuda:0')\n",
            "validation or test loss is tensor(0.0463, device='cuda:0')\n",
            "validation or test loss is tensor(0.0228, device='cuda:0')\n",
            "validation or test loss is tensor(0.0222, device='cuda:0')\n",
            "validation or test loss is tensor(0.0201, device='cuda:0')\n",
            "validation or test loss is tensor(0.0192, device='cuda:0')\n",
            "validation or test loss is tensor(0.0787, device='cuda:0')\n",
            "validation or test loss is tensor(0.0450, device='cuda:0')\n",
            "validation or test loss is tensor(0.0481, device='cuda:0')\n",
            "validation or test loss is tensor(0.0371, device='cuda:0')\n",
            "validation or test loss is tensor(0.0340, device='cuda:0')\n",
            "validation or test loss is tensor(0.0656, device='cuda:0')\n",
            "validation or test loss is tensor(0.0852, device='cuda:0')\n",
            "validation or test loss is tensor(0.0687, device='cuda:0')\n",
            "validation or test loss is tensor(0.0182, device='cuda:0')\n",
            "validation or test loss is tensor(0.0135, device='cuda:0')\n",
            "validation or test loss is tensor(0.0235, device='cuda:0')\n",
            "validation or test loss is tensor(0.0389, device='cuda:0')\n",
            "validation or test loss is tensor(0.0369, device='cuda:0')\n",
            "validation or test loss is tensor(0.0719, device='cuda:0')\n",
            "validation or test loss is tensor(0.1389, device='cuda:0')\n",
            "validation or test loss is tensor(0.0244, device='cuda:0')\n",
            "validation or test loss is tensor(0.0167, device='cuda:0')\n",
            "validation or test loss is tensor(0.0142, device='cuda:0')\n",
            "validation or test loss is tensor(0.0149, device='cuda:0')\n",
            "validation or test loss is tensor(0.0217, device='cuda:0')\n",
            "validation or test loss is tensor(0.0150, device='cuda:0')\n",
            "validation or test loss is tensor(0.0141, device='cuda:0')\n",
            "validation or test loss is tensor(0.0203, device='cuda:0')\n",
            "validation or test loss is tensor(0.0152, device='cuda:0')\n",
            "validation or test loss is tensor(0.0247, device='cuda:0')\n",
            "validation or test loss is tensor(0.1019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0182, device='cuda:0')\n",
            "validation or test loss is tensor(0.0159, device='cuda:0')\n",
            "validation or test loss is tensor(0.0370, device='cuda:0')\n",
            "validation or test loss is tensor(0.0281, device='cuda:0')\n",
            "validation or test loss is tensor(0.0254, device='cuda:0')\n",
            "validation or test loss is tensor(0.0203, device='cuda:0')\n",
            "validation or test loss is tensor(0.0239, device='cuda:0')\n",
            "validation or test loss is tensor(0.0714, device='cuda:0')\n",
            "validation or test loss is tensor(0.0597, device='cuda:0')\n",
            "validation or test loss is tensor(0.0297, device='cuda:0')\n",
            "validation or test loss is tensor(0.0449, device='cuda:0')\n",
            "validation or test loss is tensor(0.0392, device='cuda:0')\n",
            "validation or test loss is tensor(0.0525, device='cuda:0')\n",
            "validation or test loss is tensor(0.3286, device='cuda:0')\n",
            "validation or test loss is tensor(0.1614, device='cuda:0')\n",
            "validation or test loss is tensor(0.0161, device='cuda:0')\n",
            "validation or test loss is tensor(0.0147, device='cuda:0')\n",
            "validation or test loss is tensor(0.0127, device='cuda:0')\n",
            "validation or test loss is tensor(0.0114, device='cuda:0')\n",
            "validation or test loss is tensor(0.0126, device='cuda:0')\n",
            "validation or test loss is tensor(0.0106, device='cuda:0')\n",
            "validation or test loss is tensor(0.0269, device='cuda:0')\n",
            "validation or test loss is tensor(0.0184, device='cuda:0')\n",
            "validation or test loss is tensor(0.0137, device='cuda:0')\n",
            "validation or test loss is tensor(0.0111, device='cuda:0')\n",
            "validation or test loss is tensor(0.0135, device='cuda:0')\n",
            "validation or test loss is tensor(0.0120, device='cuda:0')\n",
            "validation or test loss is tensor(0.0280, device='cuda:0')\n",
            "validation or test loss is tensor(0.0109, device='cuda:0')\n",
            "validation or test loss is tensor(0.0130, device='cuda:0')\n",
            "validation or test loss is tensor(0.0158, device='cuda:0')\n",
            "validation or test loss is tensor(0.0130, device='cuda:0')\n",
            "validation or test loss is tensor(0.0519, device='cuda:0')\n",
            "validation or test loss is tensor(0.0205, device='cuda:0')\n",
            "validation or test loss is tensor(0.0253, device='cuda:0')\n",
            "validation or test loss is tensor(0.0126, device='cuda:0')\n",
            "validation or test loss is tensor(0.0206, device='cuda:0')\n",
            "validation or test loss is tensor(0.0136, device='cuda:0')\n",
            "validation or test loss is tensor(0.0144, device='cuda:0')\n",
            "validation or test loss is tensor(0.0353, device='cuda:0')\n",
            "validation or test loss is tensor(0.1132, device='cuda:0')\n",
            "validation or test loss is tensor(0.0145, device='cuda:0')\n",
            "validation or test loss is tensor(0.0146, device='cuda:0')\n",
            "validation or test loss is tensor(0.0164, device='cuda:0')\n",
            "validation or test loss is tensor(0.0103, device='cuda:0')\n",
            "validation or test loss is tensor(0.0227, device='cuda:0')\n",
            "validation or test loss is tensor(0.0092, device='cuda:0')\n",
            "validation or test loss is tensor(0.0214, device='cuda:0')\n",
            "validation or test loss is tensor(0.0212, device='cuda:0')\n",
            "validation or test loss is tensor(0.0114, device='cuda:0')\n",
            "validation or test loss is tensor(0.0162, device='cuda:0')\n",
            "validation or test loss is tensor(0.0104, device='cuda:0')\n",
            "validation or test loss is tensor(0.0943, device='cuda:0')\n",
            "validation or test loss is tensor(0.0635, device='cuda:0')\n",
            "validation or test loss is tensor(0.0126, device='cuda:0')\n",
            "validation or test loss is tensor(0.0138, device='cuda:0')\n",
            "validation or test loss is tensor(0.0088, device='cuda:0')\n",
            "validation or test loss is tensor(0.0649, device='cuda:0')\n",
            "validation or test loss is tensor(0.0110, device='cuda:0')\n",
            "validation or test loss is tensor(0.0152, device='cuda:0')\n",
            "validation or test loss is tensor(0.0184, device='cuda:0')\n",
            "validation or test loss is tensor(0.0281, device='cuda:0')\n",
            "validation or test loss is tensor(0.0092, device='cuda:0')\n",
            "validation or test loss is tensor(0.0105, device='cuda:0')\n",
            "validation or test loss is tensor(0.0095, device='cuda:0')\n",
            "validation or test loss is tensor(0.0085, device='cuda:0')\n",
            "validation or test loss is tensor(0.5190, device='cuda:0')\n",
            "validation or test loss is tensor(0.0525, device='cuda:0')\n",
            "validation or test loss is tensor(0.0195, device='cuda:0')\n",
            "validation or test loss is tensor(0.0079, device='cuda:0')\n",
            "validation or test loss is tensor(0.0150, device='cuda:0')\n",
            "validation or test loss is tensor(0.0140, device='cuda:0')\n",
            "validation or test loss is tensor(0.0123, device='cuda:0')\n",
            "validation or test loss is tensor(0.0097, device='cuda:0')\n",
            "validation or test loss is tensor(0.0145, device='cuda:0')\n",
            "validation or test loss is tensor(0.0096, device='cuda:0')\n",
            "validation or test loss is tensor(0.0118, device='cuda:0')\n",
            "validation or test loss is tensor(0.0114, device='cuda:0')\n",
            "validation or test loss is tensor(0.0206, device='cuda:0')\n",
            "validation or test loss is tensor(0.0638, device='cuda:0')\n",
            "validation or test loss is tensor(0.0513, device='cuda:0')\n",
            "validation or test loss is tensor(0.0376, device='cuda:0')\n",
            "validation or test loss is tensor(0.0120, device='cuda:0')\n",
            "validation or test loss is tensor(0.0267, device='cuda:0')\n",
            "validation or test loss is tensor(0.0073, device='cuda:0')\n",
            "validation or test loss is tensor(0.0159, device='cuda:0')\n",
            "validation or test loss is tensor(0.0210, device='cuda:0')\n",
            "validation or test loss is tensor(0.0223, device='cuda:0')\n",
            "validation or test loss is tensor(0.0077, device='cuda:0')\n",
            "validation or test loss is tensor(0.0071, device='cuda:0')\n",
            "validation or test loss is tensor(0.0122, device='cuda:0')\n",
            "validation or test loss is tensor(0.0085, device='cuda:0')\n",
            "validation or test loss is tensor(0.0482, device='cuda:0')\n",
            "validation or test loss is tensor(0.0074, device='cuda:0')\n",
            "validation or test loss is tensor(0.0455, device='cuda:0')\n",
            "validation or test loss is tensor(0.1503, device='cuda:0')\n",
            "validation or test loss is tensor(0.0961, device='cuda:0')\n",
            "validation or test loss is tensor(0.0142, device='cuda:0')\n",
            "validation or test loss is tensor(0.0093, device='cuda:0')\n",
            "validation or test loss is tensor(0.0093, device='cuda:0')\n",
            "validation or test loss is tensor(0.3410, device='cuda:0')\n",
            "validation or test loss is tensor(0.0063, device='cuda:0')\n",
            "validation or test loss is tensor(0.0074, device='cuda:0')\n",
            "validation or test loss is tensor(0.0088, device='cuda:0')\n",
            "validation or test loss is tensor(0.0063, device='cuda:0')\n",
            "validation or test loss is tensor(0.0091, device='cuda:0')\n",
            "validation or test loss is tensor(0.0062, device='cuda:0')\n",
            "validation or test loss is tensor(0.0078, device='cuda:0')\n",
            "validation or test loss is tensor(0.0130, device='cuda:0')\n",
            "validation or test loss is tensor(0.0078, device='cuda:0')\n",
            "validation or test loss is tensor(0.0258, device='cuda:0')\n",
            "validation or test loss is tensor(0.0056, device='cuda:0')\n",
            "validation or test loss is tensor(0.0058, device='cuda:0')\n",
            "validation or test loss is tensor(0.0202, device='cuda:0')\n",
            "validation or test loss is tensor(0.0695, device='cuda:0')\n",
            "validation or test loss is tensor(0.0368, device='cuda:0')\n",
            "validation or test loss is tensor(0.0095, device='cuda:0')\n",
            "validation or test loss is tensor(0.0281, device='cuda:0')\n",
            "validation or test loss is tensor(0.0084, device='cuda:0')\n",
            "validation or test loss is tensor(0.0088, device='cuda:0')\n",
            "validation or test loss is tensor(0.0083, device='cuda:0')\n",
            "validation or test loss is tensor(0.0164, device='cuda:0')\n",
            "validation or test loss is tensor(0.2416, device='cuda:0')\n",
            "validation or test loss is tensor(0.0368, device='cuda:0')\n",
            "validation or test loss is tensor(0.0108, device='cuda:0')\n",
            "validation or test loss is tensor(0.0092, device='cuda:0')\n",
            "validation or test loss is tensor(0.0053, device='cuda:0')\n",
            "validation or test loss is tensor(0.0140, device='cuda:0')\n",
            "validation or test loss is tensor(0.0101, device='cuda:0')\n",
            "validation or test loss is tensor(0.0072, device='cuda:0')\n",
            "validation or test loss is tensor(0.0096, device='cuda:0')\n",
            "validation or test loss is tensor(0.0146, device='cuda:0')\n",
            "validation or test loss is tensor(0.0091, device='cuda:0')\n",
            "validation or test loss is tensor(0.0062, device='cuda:0')\n",
            "validation or test loss is tensor(0.0066, device='cuda:0')\n",
            "validation or test loss is tensor(0.0084, device='cuda:0')\n",
            "validation or test loss is tensor(0.0922, device='cuda:0')\n",
            "validation or test loss is tensor(0.0193, device='cuda:0')\n",
            "validation or test loss is tensor(0.0273, device='cuda:0')\n",
            "validation or test loss is tensor(0.0105, device='cuda:0')\n",
            "validation or test loss is tensor(0.0113, device='cuda:0')\n",
            "validation or test loss is tensor(0.0095, device='cuda:0')\n",
            "validation or test loss is tensor(0.0066, device='cuda:0')\n",
            "validation or test loss is tensor(0.0073, device='cuda:0')\n",
            "validation or test loss is tensor(0.0068, device='cuda:0')\n",
            "validation or test loss is tensor(0.0152, device='cuda:0')\n",
            "validation or test loss is tensor(0.1212, device='cuda:0')\n",
            "validation or test loss is tensor(0.0080, device='cuda:0')\n",
            "validation or test loss is tensor(0.0175, device='cuda:0')\n",
            "validation or test loss is tensor(0.0122, device='cuda:0')\n",
            "validation or test loss is tensor(0.0067, device='cuda:0')\n",
            "validation or test loss is tensor(0.0079, device='cuda:0')\n",
            "validation or test loss is tensor(0.0080, device='cuda:0')\n",
            "validation or test loss is tensor(0.0084, device='cuda:0')\n",
            "validation or test loss is tensor(0.0166, device='cuda:0')\n",
            "validation or test loss is tensor(0.0109, device='cuda:0')\n",
            "validation or test loss is tensor(0.0091, device='cuda:0')\n",
            "validation or test loss is tensor(0.0177, device='cuda:0')\n",
            "validation or test loss is tensor(0.0148, device='cuda:0')\n",
            "validation or test loss is tensor(0.0303, device='cuda:0')\n",
            "validation or test loss is tensor(0.0875, device='cuda:0')\n",
            "validation or test loss is tensor(0.0471, device='cuda:0')\n",
            "validation or test loss is tensor(0.0084, device='cuda:0')\n",
            "validation or test loss is tensor(0.0059, device='cuda:0')\n",
            "validation or test loss is tensor(0.0445, device='cuda:0')\n",
            "validation or test loss is tensor(0.0193, device='cuda:0')\n",
            "validation or test loss is tensor(0.0063, device='cuda:0')\n",
            "validation or test loss is tensor(0.0048, device='cuda:0')\n",
            "validation or test loss is tensor(0.0214, device='cuda:0')\n",
            "validation or test loss is tensor(0.0059, device='cuda:0')\n",
            "validation or test loss is tensor(0.0084, device='cuda:0')\n",
            "validation or test loss is tensor(0.0294, device='cuda:0')\n",
            "validation or test loss is tensor(0.0103, device='cuda:0')\n",
            "validation or test loss is tensor(0.0207, device='cuda:0')\n",
            "validation or test loss is tensor(0.0563, device='cuda:0')\n",
            "validation or test loss is tensor(0.0131, device='cuda:0')\n",
            "validation or test loss is tensor(0.0047, device='cuda:0')\n",
            "validation or test loss is tensor(0.0132, device='cuda:0')\n",
            "validation or test loss is tensor(0.0065, device='cuda:0')\n",
            "validation or test loss is tensor(0.0100, device='cuda:0')\n",
            "validation or test loss is tensor(0.0081, device='cuda:0')\n",
            "validation or test loss is tensor(0.0583, device='cuda:0')\n",
            "validation or test loss is tensor(0.0058, device='cuda:0')\n",
            "validation or test loss is tensor(0.0071, device='cuda:0')\n",
            "validation or test loss is tensor(0.0155, device='cuda:0')\n",
            "validation or test loss is tensor(0.0372, device='cuda:0')\n",
            "validation or test loss is tensor(0.0366, device='cuda:0')\n",
            "validation or test loss is tensor(0.0061, device='cuda:0')\n",
            "validation or test loss is tensor(0.0113, device='cuda:0')\n",
            "validation or test loss is tensor(0.0060, device='cuda:0')\n",
            "validation or test loss is tensor(0.0706, device='cuda:0')\n",
            "validation or test loss is tensor(0.0045, device='cuda:0')\n",
            "validation or test loss is tensor(0.0052, device='cuda:0')\n",
            "validation or test loss is tensor(0.1194, device='cuda:0')\n",
            "validation or test loss is tensor(0.0206, device='cuda:0')\n",
            "validation or test loss is tensor(0.0110, device='cuda:0')\n",
            "validation or test loss is tensor(0.0057, device='cuda:0')\n",
            "validation or test loss is tensor(0.0046, device='cuda:0')\n",
            "validation or test loss is tensor(0.0056, device='cuda:0')\n",
            "validation or test loss is tensor(0.0061, device='cuda:0')\n",
            "validation or test loss is tensor(0.0070, device='cuda:0')\n",
            "validation or test loss is tensor(0.0105, device='cuda:0')\n",
            "validation or test loss is tensor(0.0058, device='cuda:0')\n",
            "validation or test loss is tensor(0.0053, device='cuda:0')\n",
            "validation or test loss is tensor(0.0074, device='cuda:0')\n",
            "validation or test loss is tensor(0.0068, device='cuda:0')\n",
            "validation or test loss is tensor(0.0134, device='cuda:0')\n",
            "validation or test loss is tensor(0.0142, device='cuda:0')\n",
            "validation or test loss is tensor(0.1342, device='cuda:0')\n",
            "validation or test loss is tensor(0.0279, device='cuda:0')\n",
            "validation or test loss is tensor(0.0050, device='cuda:0')\n",
            "validation or test loss is tensor(0.0038, device='cuda:0')\n",
            "validation or test loss is tensor(0.0098, device='cuda:0')\n",
            "validation or test loss is tensor(0.0174, device='cuda:0')\n",
            "validation or test loss is tensor(0.0091, device='cuda:0')\n",
            "validation or test loss is tensor(0.0060, device='cuda:0')\n",
            "validation or test loss is tensor(0.0547, device='cuda:0')\n",
            "validation or test loss is tensor(0.0176, device='cuda:0')\n",
            "validation or test loss is tensor(0.0042, device='cuda:0')\n",
            "validation or test loss is tensor(0.0046, device='cuda:0')\n",
            "validation or test loss is tensor(0.0083, device='cuda:0')\n",
            "validation or test loss is tensor(0.0048, device='cuda:0')\n",
            "validation or test loss is tensor(0.0075, device='cuda:0')\n",
            "validation or test loss is tensor(0.0041, device='cuda:0')\n",
            "validation or test loss is tensor(0.0039, device='cuda:0')\n",
            "validation or test loss is tensor(0.0139, device='cuda:0')\n",
            "validation or test loss is tensor(0.0282, device='cuda:0')\n",
            "validation or test loss is tensor(0.0326, device='cuda:0')\n",
            "validation or test loss is tensor(0.0061, device='cuda:0')\n",
            "validation or test loss is tensor(0.0114, device='cuda:0')\n",
            "validation or test loss is tensor(0.0076, device='cuda:0')\n",
            "validation or test loss is tensor(0.0083, device='cuda:0')\n",
            "validation or test loss is tensor(0.0315, device='cuda:0')\n",
            "validation or test loss is tensor(0.0049, device='cuda:0')\n",
            "validation or test loss is tensor(0.0211, device='cuda:0')\n",
            "validation or test loss is tensor(0.0079, device='cuda:0')\n",
            "validation or test loss is tensor(0.0075, device='cuda:0')\n",
            "validation or test loss is tensor(0.0059, device='cuda:0')\n",
            "validation or test loss is tensor(0.0158, device='cuda:0')\n",
            "validation or test loss is tensor(0.0254, device='cuda:0')\n",
            "validation or test loss is tensor(0.0105, device='cuda:0')\n",
            "validation or test loss is tensor(0.0058, device='cuda:0')\n",
            "validation or test loss is tensor(0.0045, device='cuda:0')\n",
            "validation or test loss is tensor(0.0041, device='cuda:0')\n",
            "validation or test loss is tensor(0.0104, device='cuda:0')\n",
            "validation or test loss is tensor(0.0122, device='cuda:0')\n",
            "validation or test loss is tensor(0.0276, device='cuda:0')\n",
            "validation or test loss is tensor(0.0091, device='cuda:0')\n",
            "validation or test loss is tensor(0.0115, device='cuda:0')\n",
            "validation or test loss is tensor(0.0033, device='cuda:0')\n",
            "validation or test loss is tensor(0.0248, device='cuda:0')\n",
            "validation or test loss is tensor(0.0480, device='cuda:0')\n",
            "validation or test loss is tensor(0.0217, device='cuda:0')\n",
            "validation or test loss is tensor(0.0039, device='cuda:0')\n",
            "validation or test loss is tensor(0.0137, device='cuda:0')\n",
            "validation or test loss is tensor(0.0050, device='cuda:0')\n",
            "validation or test loss is tensor(0.0172, device='cuda:0')\n",
            "validation or test loss is tensor(0.0385, device='cuda:0')\n",
            "validation or test loss is tensor(0.0054, device='cuda:0')\n",
            "validation or test loss is tensor(0.0030, device='cuda:0')\n",
            "validation or test loss is tensor(0.0036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0056, device='cuda:0')\n",
            "validation or test loss is tensor(0.0512, device='cuda:0')\n",
            "validation or test loss is tensor(0.0186, device='cuda:0')\n",
            "validation or test loss is tensor(0.0126, device='cuda:0')\n",
            "validation or test loss is tensor(0.0035, device='cuda:0')\n",
            "validation or test loss is tensor(0.0093, device='cuda:0')\n",
            "validation or test loss is tensor(0.0035, device='cuda:0')\n",
            "validation or test loss is tensor(0.0131, device='cuda:0')\n",
            "validation or test loss is tensor(0.0131, device='cuda:0')\n",
            "validation or test loss is tensor(0.0082, device='cuda:0')\n",
            "validation or test loss is tensor(0.0069, device='cuda:0')\n",
            "validation or test loss is tensor(0.0053, device='cuda:0')\n",
            "validation or test loss is tensor(0.0038, device='cuda:0')\n",
            "validation or test loss is tensor(0.0043, device='cuda:0')\n",
            "validation or test loss is tensor(0.0147, device='cuda:0')\n",
            "validation or test loss is tensor(0.0521, device='cuda:0')\n",
            "validation or test loss is tensor(0.0146, device='cuda:0')\n",
            "validation or test loss is tensor(0.0067, device='cuda:0')\n",
            "validation or test loss is tensor(0.0260, device='cuda:0')\n",
            "validation or test loss is tensor(0.0036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0113, device='cuda:0')\n",
            "validation or test loss is tensor(0.0048, device='cuda:0')\n",
            "validation or test loss is tensor(0.0046, device='cuda:0')\n",
            "validation or test loss is tensor(0.0079, device='cuda:0')\n",
            "validation or test loss is tensor(0.0052, device='cuda:0')\n",
            "validation or test loss is tensor(0.0055, device='cuda:0')\n",
            "validation or test loss is tensor(0.0068, device='cuda:0')\n",
            "validation or test loss is tensor(0.0064, device='cuda:0')\n",
            "validation or test loss is tensor(0.0200, device='cuda:0')\n",
            "validation or test loss is tensor(0.0250, device='cuda:0')\n",
            "validation or test loss is tensor(0.0178, device='cuda:0')\n",
            "validation or test loss is tensor(0.0194, device='cuda:0')\n",
            "validation or test loss is tensor(0.0165, device='cuda:0')\n",
            "validation or test loss is tensor(0.0263, device='cuda:0')\n",
            "validation or test loss is tensor(0.0041, device='cuda:0')\n",
            "validation or test loss is tensor(0.0036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0304, device='cuda:0')\n",
            "validation or test loss is tensor(0.0144, device='cuda:0')\n",
            "validation or test loss is tensor(0.0045, device='cuda:0')\n",
            "validation or test loss is tensor(0.0220, device='cuda:0')\n",
            "validation or test loss is tensor(0.0058, device='cuda:0')\n",
            "validation or test loss is tensor(0.0357, device='cuda:0')\n",
            "validation or test loss is tensor(0.0078, device='cuda:0')\n",
            "validation or test loss is tensor(0.0031, device='cuda:0')\n",
            "validation or test loss is tensor(0.0037, device='cuda:0')\n",
            "validation or test loss is tensor(0.0039, device='cuda:0')\n",
            "validation or test loss is tensor(0.0047, device='cuda:0')\n",
            "validation or test loss is tensor(0.0080, device='cuda:0')\n",
            "validation or test loss is tensor(0.0167, device='cuda:0')\n",
            "validation or test loss is tensor(0.0038, device='cuda:0')\n",
            "validation or test loss is tensor(0.0136, device='cuda:0')\n",
            "validation or test loss is tensor(0.0047, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0825, device='cuda:0')\n",
            "validation or test loss is tensor(0.0054, device='cuda:0')\n",
            "validation or test loss is tensor(0.0128, device='cuda:0')\n",
            "validation or test loss is tensor(0.0135, device='cuda:0')\n",
            "validation or test loss is tensor(0.0037, device='cuda:0')\n",
            "validation or test loss is tensor(0.0057, device='cuda:0')\n",
            "validation or test loss is tensor(0.0182, device='cuda:0')\n",
            "validation or test loss is tensor(0.0568, device='cuda:0')\n",
            "validation or test loss is tensor(0.0204, device='cuda:0')\n",
            "validation or test loss is tensor(0.0036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0062, device='cuda:0')\n",
            "validation or test loss is tensor(0.0025, device='cuda:0')\n",
            "validation or test loss is tensor(0.0044, device='cuda:0')\n",
            "validation or test loss is tensor(0.0131, device='cuda:0')\n",
            "validation or test loss is tensor(0.0110, device='cuda:0')\n",
            "validation or test loss is tensor(0.0398, device='cuda:0')\n",
            "validation or test loss is tensor(0.0033, device='cuda:0')\n",
            "validation or test loss is tensor(0.0057, device='cuda:0')\n",
            "validation or test loss is tensor(0.0050, device='cuda:0')\n",
            "validation or test loss is tensor(0.0035, device='cuda:0')\n",
            "validation or test loss is tensor(0.0034, device='cuda:0')\n",
            "validation or test loss is tensor(0.0039, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0150, device='cuda:0')\n",
            "validation or test loss is tensor(0.0112, device='cuda:0')\n",
            "validation or test loss is tensor(0.0150, device='cuda:0')\n",
            "validation or test loss is tensor(0.0119, device='cuda:0')\n",
            "validation or test loss is tensor(0.0057, device='cuda:0')\n",
            "validation or test loss is tensor(0.0115, device='cuda:0')\n",
            "validation or test loss is tensor(0.0057, device='cuda:0')\n",
            "validation or test loss is tensor(0.0042, device='cuda:0')\n",
            "validation or test loss is tensor(0.0152, device='cuda:0')\n",
            "validation or test loss is tensor(0.0199, device='cuda:0')\n",
            "validation or test loss is tensor(0.0189, device='cuda:0')\n",
            "validation or test loss is tensor(0.0159, device='cuda:0')\n",
            "validation or test loss is tensor(0.0099, device='cuda:0')\n",
            "validation or test loss is tensor(0.0025, device='cuda:0')\n",
            "validation or test loss is tensor(0.0048, device='cuda:0')\n",
            "validation or test loss is tensor(0.0064, device='cuda:0')\n",
            "validation or test loss is tensor(0.0043, device='cuda:0')\n",
            "validation or test loss is tensor(0.0061, device='cuda:0')\n",
            "validation or test loss is tensor(0.0035, device='cuda:0')\n",
            "validation or test loss is tensor(0.0023, device='cuda:0')\n",
            "validation or test loss is tensor(0.0133, device='cuda:0')\n",
            "validation or test loss is tensor(0.0030, device='cuda:0')\n",
            "validation or test loss is tensor(0.0137, device='cuda:0')\n",
            "validation or test loss is tensor(0.0054, device='cuda:0')\n",
            "validation or test loss is tensor(0.0020, device='cuda:0')\n",
            "validation or test loss is tensor(0.0031, device='cuda:0')\n",
            "validation or test loss is tensor(0.0176, device='cuda:0')\n",
            "validation or test loss is tensor(0.0024, device='cuda:0')\n",
            "validation or test loss is tensor(0.0058, device='cuda:0')\n",
            "validation or test loss is tensor(0.1024, device='cuda:0')\n",
            "validation or test loss is tensor(0.0038, device='cuda:0')\n",
            "validation or test loss is tensor(0.0060, device='cuda:0')\n",
            "validation or test loss is tensor(0.0032, device='cuda:0')\n",
            "validation or test loss is tensor(0.0036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0034, device='cuda:0')\n",
            "validation or test loss is tensor(0.0055, device='cuda:0')\n",
            "validation or test loss is tensor(0.0026, device='cuda:0')\n",
            "validation or test loss is tensor(0.0074, device='cuda:0')\n",
            "validation or test loss is tensor(0.0043, device='cuda:0')\n",
            "validation or test loss is tensor(0.0124, device='cuda:0')\n",
            "validation or test loss is tensor(0.0141, device='cuda:0')\n",
            "validation or test loss is tensor(0.0056, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0093, device='cuda:0')\n",
            "validation or test loss is tensor(0.0041, device='cuda:0')\n",
            "validation or test loss is tensor(0.0156, device='cuda:0')\n",
            "validation or test loss is tensor(0.0063, device='cuda:0')\n",
            "validation or test loss is tensor(0.0029, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0167, device='cuda:0')\n",
            "validation or test loss is tensor(0.0062, device='cuda:0')\n",
            "validation or test loss is tensor(0.0181, device='cuda:0')\n",
            "validation or test loss is tensor(0.0050, device='cuda:0')\n",
            "validation or test loss is tensor(0.0031, device='cuda:0')\n",
            "validation or test loss is tensor(0.0026, device='cuda:0')\n",
            "validation or test loss is tensor(0.0031, device='cuda:0')\n",
            "validation or test loss is tensor(0.0160, device='cuda:0')\n",
            "validation or test loss is tensor(0.0057, device='cuda:0')\n",
            "validation or test loss is tensor(0.0032, device='cuda:0')\n",
            "validation or test loss is tensor(0.0130, device='cuda:0')\n",
            "validation or test loss is tensor(0.1254, device='cuda:0')\n",
            "validation or test loss is tensor(0.0134, device='cuda:0')\n",
            "validation or test loss is tensor(0.0026, device='cuda:0')\n",
            "validation or test loss is tensor(0.0027, device='cuda:0')\n",
            "validation or test loss is tensor(0.0026, device='cuda:0')\n",
            "validation or test loss is tensor(0.0035, device='cuda:0')\n",
            "validation or test loss is tensor(0.0080, device='cuda:0')\n",
            "validation or test loss is tensor(0.0028, device='cuda:0')\n",
            "validation or test loss is tensor(0.0046, device='cuda:0')\n",
            "validation or test loss is tensor(0.0032, device='cuda:0')\n",
            "validation or test loss is tensor(0.0087, device='cuda:0')\n",
            "validation or test loss is tensor(0.0026, device='cuda:0')\n",
            "validation or test loss is tensor(0.0030, device='cuda:0')\n",
            "validation or test loss is tensor(0.0023, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0062, device='cuda:0')\n",
            "validation or test loss is tensor(0.0029, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0042, device='cuda:0')\n",
            "validation or test loss is tensor(0.0020, device='cuda:0')\n",
            "validation or test loss is tensor(0.0028, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0080, device='cuda:0')\n",
            "validation or test loss is tensor(0.0026, device='cuda:0')\n",
            "validation or test loss is tensor(0.0026, device='cuda:0')\n",
            "validation or test loss is tensor(0.0085, device='cuda:0')\n",
            "validation or test loss is tensor(0.0335, device='cuda:0')\n",
            "validation or test loss is tensor(0.0023, device='cuda:0')\n",
            "validation or test loss is tensor(0.0055, device='cuda:0')\n",
            "validation or test loss is tensor(0.0026, device='cuda:0')\n",
            "validation or test loss is tensor(0.0062, device='cuda:0')\n",
            "validation or test loss is tensor(0.0063, device='cuda:0')\n",
            "validation or test loss is tensor(0.0076, device='cuda:0')\n",
            "validation or test loss is tensor(0.0041, device='cuda:0')\n",
            "validation or test loss is tensor(0.0023, device='cuda:0')\n",
            "validation or test loss is tensor(0.0031, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0050, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0065, device='cuda:0')\n",
            "validation or test loss is tensor(0.0106, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0090, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0053, device='cuda:0')\n",
            "validation or test loss is tensor(0.0027, device='cuda:0')\n",
            "validation or test loss is tensor(0.0042, device='cuda:0')\n",
            "validation or test loss is tensor(0.0087, device='cuda:0')\n",
            "validation or test loss is tensor(0.0027, device='cuda:0')\n",
            "validation or test loss is tensor(0.0030, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0648, device='cuda:0')\n",
            "validation or test loss is tensor(0.0015, device='cuda:0')\n",
            "validation or test loss is tensor(0.0042, device='cuda:0')\n",
            "validation or test loss is tensor(0.0017, device='cuda:0')\n",
            "validation or test loss is tensor(0.0081, device='cuda:0')\n",
            "validation or test loss is tensor(0.0084, device='cuda:0')\n",
            "validation or test loss is tensor(0.0020, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0026, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0027, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0023, device='cuda:0')\n",
            "validation or test loss is tensor(0.0034, device='cuda:0')\n",
            "validation or test loss is tensor(0.0219, device='cuda:0')\n",
            "validation or test loss is tensor(0.0053, device='cuda:0')\n",
            "validation or test loss is tensor(0.0020, device='cuda:0')\n",
            "validation or test loss is tensor(0.0027, device='cuda:0')\n",
            "validation or test loss is tensor(0.0090, device='cuda:0')\n",
            "validation or test loss is tensor(0.0067, device='cuda:0')\n",
            "validation or test loss is tensor(0.0049, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0025, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0108, device='cuda:0')\n",
            "validation or test loss is tensor(0.0068, device='cuda:0')\n",
            "validation or test loss is tensor(0.0034, device='cuda:0')\n",
            "validation or test loss is tensor(0.0024, device='cuda:0')\n",
            "validation or test loss is tensor(0.0044, device='cuda:0')\n",
            "validation or test loss is tensor(0.0032, device='cuda:0')\n",
            "validation or test loss is tensor(0.0056, device='cuda:0')\n",
            "validation or test loss is tensor(0.1066, device='cuda:0')\n",
            "validation or test loss is tensor(0.0172, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0015, device='cuda:0')\n",
            "validation or test loss is tensor(0.0017, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0027, device='cuda:0')\n",
            "validation or test loss is tensor(0.0045, device='cuda:0')\n",
            "validation or test loss is tensor(0.0040, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0038, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0106, device='cuda:0')\n",
            "validation or test loss is tensor(0.0030, device='cuda:0')\n",
            "validation or test loss is tensor(0.0093, device='cuda:0')\n",
            "validation or test loss is tensor(0.0039, device='cuda:0')\n",
            "validation or test loss is tensor(0.0024, device='cuda:0')\n",
            "validation or test loss is tensor(0.0072, device='cuda:0')\n",
            "validation or test loss is tensor(0.0134, device='cuda:0')\n",
            "validation or test loss is tensor(0.0048, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0024, device='cuda:0')\n",
            "validation or test loss is tensor(0.0058, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0016, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0017, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0224, device='cuda:0')\n",
            "validation or test loss is tensor(0.0228, device='cuda:0')\n",
            "validation or test loss is tensor(0.0069, device='cuda:0')\n",
            "validation or test loss is tensor(0.0068, device='cuda:0')\n",
            "validation or test loss is tensor(0.0027, device='cuda:0')\n",
            "validation or test loss is tensor(0.0017, device='cuda:0')\n",
            "validation or test loss is tensor(0.0041, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0039, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0072, device='cuda:0')\n",
            "validation or test loss is tensor(0.0023, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0025, device='cuda:0')\n",
            "validation or test loss is tensor(0.0036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0027, device='cuda:0')\n",
            "validation or test loss is tensor(0.0023, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0021, device='cuda:0')\n",
            "validation or test loss is tensor(0.0040, device='cuda:0')\n",
            "validation or test loss is tensor(0.0016, device='cuda:0')\n",
            "validation or test loss is tensor(0.0119, device='cuda:0')\n",
            "validation or test loss is tensor(0.0027, device='cuda:0')\n",
            "validation or test loss is tensor(0.0034, device='cuda:0')\n",
            "validation or test loss is tensor(0.0134, device='cuda:0')\n",
            "validation or test loss is tensor(0.0036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0016, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0033, device='cuda:0')\n",
            "validation or test loss is tensor(0.0028, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0016, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0015, device='cuda:0')\n",
            "validation or test loss is tensor(0.0016, device='cuda:0')\n",
            "validation or test loss is tensor(0.0400, device='cuda:0')\n",
            "validation or test loss is tensor(0.0046, device='cuda:0')\n",
            "validation or test loss is tensor(0.0176, device='cuda:0')\n",
            "validation or test loss is tensor(0.0024, device='cuda:0')\n",
            "validation or test loss is tensor(0.0052, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0029, device='cuda:0')\n",
            "validation or test loss is tensor(0.0036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0025, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0247, device='cuda:0')\n",
            "validation or test loss is tensor(0.0041, device='cuda:0')\n",
            "validation or test loss is tensor(0.0015, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0020, device='cuda:0')\n",
            "validation or test loss is tensor(0.0017, device='cuda:0')\n",
            "validation or test loss is tensor(0.0015, device='cuda:0')\n",
            "validation or test loss is tensor(0.0026, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0031, device='cuda:0')\n",
            "validation or test loss is tensor(0.0063, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0028, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0027, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0047, device='cuda:0')\n",
            "validation or test loss is tensor(0.0078, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0024, device='cuda:0')\n",
            "validation or test loss is tensor(0.0080, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0081, device='cuda:0')\n",
            "validation or test loss is tensor(0.0009, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0033, device='cuda:0')\n",
            "validation or test loss is tensor(0.0092, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0016, device='cuda:0')\n",
            "validation or test loss is tensor(0.0030, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0016, device='cuda:0')\n",
            "validation or test loss is tensor(0.0052, device='cuda:0')\n",
            "validation or test loss is tensor(0.0009, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0239, device='cuda:0')\n",
            "validation or test loss is tensor(0.0020, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0046, device='cuda:0')\n",
            "validation or test loss is tensor(0.0058, device='cuda:0')\n",
            "validation or test loss is tensor(0.0074, device='cuda:0')\n",
            "validation or test loss is tensor(0.0039, device='cuda:0')\n",
            "validation or test loss is tensor(0.0038, device='cuda:0')\n",
            "validation or test loss is tensor(0.0187, device='cuda:0')\n",
            "validation or test loss is tensor(0.0047, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0023, device='cuda:0')\n",
            "validation or test loss is tensor(0.0017, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0049, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0069, device='cuda:0')\n",
            "validation or test loss is tensor(0.0024, device='cuda:0')\n",
            "validation or test loss is tensor(0.0024, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0029, device='cuda:0')\n",
            "validation or test loss is tensor(0.0028, device='cuda:0')\n",
            "validation or test loss is tensor(0.0015, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0113, device='cuda:0')\n",
            "validation or test loss is tensor(0.0051, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0025, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0023, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0017, device='cuda:0')\n",
            "validation or test loss is tensor(0.0022, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0019, device='cuda:0')\n",
            "validation or test loss is tensor(0.0009, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0043, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0052, device='cuda:0')\n",
            "validation or test loss is tensor(0.0025, device='cuda:0')\n",
            "validation or test loss is tensor(0.0015, device='cuda:0')\n",
            "validation or test loss is tensor(0.0055, device='cuda:0')\n",
            "validation or test loss is tensor(0.0024, device='cuda:0')\n",
            "validation or test loss is tensor(0.0017, device='cuda:0')\n",
            "validation or test loss is tensor(0.0023, device='cuda:0')\n",
            "validation or test loss is tensor(0.0325, device='cuda:0')\n",
            "validation or test loss is tensor(0.0117, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0010, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0018, device='cuda:0')\n",
            "validation or test loss is tensor(0.0028, device='cuda:0')\n",
            "validation or test loss is tensor(0.0020, device='cuda:0')\n",
            "validation or test loss is tensor(0.0033, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0014, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0015, device='cuda:0')\n",
            "validation or test loss is tensor(0.0036, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0076, device='cuda:0')\n",
            "validation or test loss is tensor(0.0084, device='cuda:0')\n",
            "validation or test loss is tensor(0.0016, device='cuda:0')\n",
            "validation or test loss is tensor(0.0011, device='cuda:0')\n",
            "validation or test loss is tensor(0.0012, device='cuda:0')\n",
            "validation or test loss is tensor(0.0009, device='cuda:0')\n",
            "validation or test loss is tensor(0.0017, device='cuda:0')\n",
            "validation or test loss is tensor(0.0046, device='cuda:0')\n",
            "validation or test loss is tensor(0.0120, device='cuda:0')\n",
            "validation or test loss is tensor(0.0016, device='cuda:0')\n",
            "validation or test loss is tensor(0.0013, device='cuda:0')\n",
            "validation or test loss is tensor(0.0061, device='cuda:0')\n",
            "Training time with gpu is 219.10593032836914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMuQPzF6GyvL",
        "colab_type": "text"
      },
      "source": [
        "#FOR YAO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3It6tN1MfvV",
        "colab_type": "code",
        "outputId": "013687e2-8b92-41cd-a410-99d657810111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in model.state_dict().items():\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('0.weight', tensor([[-2.0458e-02, -1.5641e-01,  5.1515e-02, -9.0227e-02,  5.6205e-01,\n",
            "         -7.3779e-02, -2.6524e-02, -3.1198e-01,  1.2216e-01, -2.1926e-01,\n",
            "          1.4793e-02],\n",
            "        [-2.8699e-38, -8.5207e-07, -5.4255e-05,  1.1295e-04, -5.9715e-05,\n",
            "          8.2738e-06, -7.2094e-05, -1.1770e-04, -1.1126e-07, -1.2562e-07,\n",
            "         -1.4947e-07],\n",
            "        [ 1.2804e-02, -3.8568e-01,  1.8751e-01, -1.5994e-01,  3.7965e-01,\n",
            "         -1.8492e-01, -1.1157e-01, -1.2266e-01,  9.5627e-02,  1.7680e-01,\n",
            "         -1.1924e-01],\n",
            "        [ 1.4020e-03,  1.1861e-02,  9.0042e-02,  1.1770e-02, -1.0826e-01,\n",
            "          5.3578e-02, -3.5398e-02, -2.8603e-02,  5.9892e-02, -1.7745e-02,\n",
            "          3.0489e-02],\n",
            "        [-3.3221e-03, -2.0158e-02,  1.6976e-01, -8.4094e-02,  1.1614e-02,\n",
            "          1.2746e-02, -1.9104e-01,  7.4590e-02, -1.6939e-01,  1.1650e-01,\n",
            "          4.5592e-02],\n",
            "        [-4.5109e-03, -1.1179e-01, -2.0074e-01, -6.9110e-02,  3.5268e-01,\n",
            "         -1.1020e-01, -3.2524e-02,  5.3641e-02, -4.5945e-02,  9.1261e-02,\n",
            "         -8.7266e-02],\n",
            "        [-8.1782e-04, -1.8624e-02, -5.1035e-02, -3.6133e-01, -3.2522e-02,\n",
            "          1.1258e-02,  2.4722e-02,  3.6826e-01, -7.2225e-02,  5.9611e-02,\n",
            "         -7.8133e-02],\n",
            "        [ 9.7321e-08, -7.0393e-03, -1.0460e-02,  1.5501e-03, -1.0341e-02,\n",
            "         -9.5151e-03, -1.1581e-02, -6.8910e-03, -3.6082e-03, -3.6477e-03,\n",
            "         -3.7719e-03],\n",
            "        [ 1.3548e-02,  2.4164e-01,  1.2710e-01,  6.8907e-02, -7.3001e-02,\n",
            "          4.6999e-02,  6.4504e-02, -1.8527e-01,  2.0077e-02, -8.2421e-02,\n",
            "          2.0527e-01],\n",
            "        [-2.7739e-03,  1.6919e-02,  1.9599e-01,  1.2291e-01,  1.9970e-01,\n",
            "         -1.3523e-01, -2.4708e-01, -9.3275e-02, -3.8606e-01,  3.1379e-01,\n",
            "          1.0663e-01],\n",
            "        [-3.8469e-02, -2.2089e-01,  1.4565e-01, -1.6073e-01,  4.8270e-02,\n",
            "         -2.2009e-01, -1.3855e-02,  1.4003e-01,  7.6065e-02,  1.7868e-01,\n",
            "         -9.1534e-02],\n",
            "        [-4.4567e-04, -8.5323e-03, -1.1428e-01,  6.5399e-02, -1.8914e-02,\n",
            "         -7.3720e-02,  1.1652e-01, -3.6449e-03, -6.0275e-02,  5.9608e-02,\n",
            "          1.0659e-02],\n",
            "        [ 4.8071e-04, -5.6604e-02,  1.0218e-01, -9.8014e-03, -4.0386e-03,\n",
            "         -8.3680e-02, -1.1556e-01,  7.9545e-02, -4.3656e-02,  4.2375e-02,\n",
            "          2.1156e-02],\n",
            "        [-6.9777e-03,  5.1909e-02,  2.0961e-02, -5.6727e-02, -6.6333e-02,\n",
            "          1.6461e-01,  9.4756e-03, -8.1226e-02, -6.7122e-03, -4.3467e-02,\n",
            "          8.6604e-02],\n",
            "        [ 4.2913e-09, -2.3610e-03, -7.1417e-03, -1.5470e-02, -7.1010e-03,\n",
            "          1.5937e-03, -7.1972e-03,  5.7463e-03, -1.4487e-03, -1.5523e-03,\n",
            "         -1.7554e-03],\n",
            "        [-1.3217e-02,  1.4463e-01,  2.7270e-02, -1.4844e-01,  2.3479e-01,\n",
            "         -1.4365e-01,  5.4023e-02,  7.9439e-02, -9.6761e-02, -2.9037e-01,\n",
            "          1.0702e-01],\n",
            "        [-1.0003e-02,  9.4410e-02,  1.3048e-01, -1.7240e-01, -1.6831e-01,\n",
            "          1.6226e-01, -1.8338e-02,  4.4902e-02,  6.5806e-03,  9.6477e-02,\n",
            "          3.9972e-02],\n",
            "        [ 3.6473e-03, -1.6374e-01, -6.7152e-02, -1.0326e-01,  3.0015e-01,\n",
            "         -4.5024e-02, -2.7722e-02, -5.8548e-03,  1.0680e-02, -1.7571e-01,\n",
            "         -9.6217e-02],\n",
            "        [ 3.2261e-22, -4.3656e-03, -8.5006e-03,  2.3917e-02, -1.4687e-02,\n",
            "         -4.7778e-02, -9.4343e-03,  2.4241e-02, -4.6175e-03, -4.6203e-03,\n",
            "         -4.4899e-03],\n",
            "        [ 5.7934e-09, -4.6231e-04, -7.4476e-04, -4.0123e-05, -7.2446e-04,\n",
            "         -2.0138e-04, -8.6357e-04, -5.8877e-04, -1.7957e-04, -1.7773e-04,\n",
            "         -1.8472e-04]], device='cuda:0'))\n",
            "('0.bias', tensor([ 7.6972e-05,  6.3530e-06, -5.3064e-05,  1.2612e-05, -3.1817e-05,\n",
            "         7.3250e-06, -2.2228e-05, -8.1482e-07,  7.6063e-05,  6.0869e-05,\n",
            "         5.2297e-05, -1.1039e-05,  1.5561e-05,  1.5147e-05,  3.2899e-06,\n",
            "        -1.0642e-04,  1.0902e-04,  7.4436e-05, -2.5146e-07, -2.1343e-06],\n",
            "       device='cuda:0'))\n",
            "('2.weight', tensor([[ 3.2021e-01, -1.1088e-35, -1.9144e-01,  6.1981e-02, -5.1164e-02,\n",
            "          2.2723e-01, -1.8134e-01, -3.6779e-04, -6.3050e-02, -1.9222e-01,\n",
            "         -1.6468e-01, -2.0593e-01,  1.2027e-01, -3.7226e-02, -3.0211e-03,\n",
            "          1.1739e-01,  3.2186e-01,  2.1869e-01, -8.3692e-04,  3.5517e-05],\n",
            "        [-7.5037e-07,  1.6211e-29, -3.9639e-07, -3.0041e-08, -1.7522e-09,\n",
            "         -1.9408e-08,  3.4370e-08,  6.6887e-41, -5.2453e-08, -4.5649e-09,\n",
            "         -3.0836e-08, -6.9899e-11,  4.5555e-13,  1.3681e-08, -7.4087e-42,\n",
            "         -3.7075e-07, -7.6568e-08, -2.8840e-09, -8.5652e-41,  1.9725e-27],\n",
            "        [ 4.5362e-02,  4.2858e-09,  1.9344e-01, -2.5023e-02,  8.2096e-02,\n",
            "         -8.4960e-02,  3.0200e-01, -1.5975e-04,  6.5350e-02, -9.1147e-02,\n",
            "          1.9846e-01,  1.1915e-01, -8.4372e-02,  6.4204e-03, -5.4558e-03,\n",
            "         -4.4265e-02, -1.6446e-02, -1.5223e-02, -2.2406e-03,  1.0878e-04],\n",
            "        [-5.3085e-04, -2.4971e-40, -2.1567e-04, -4.9317e-04, -5.0396e-07,\n",
            "         -1.7583e-07, -5.3323e-05, -6.7847e-41, -5.0431e-05, -6.7688e-07,\n",
            "         -5.9524e-06, -1.6687e-41,  4.6327e-42, -1.9499e-04,  4.4166e-41,\n",
            "         -1.4829e-04, -5.6321e-04, -3.2945e-09, -2.0652e-40,  4.4561e-36],\n",
            "        [-2.8229e-02, -3.5992e-30, -2.7849e-02,  1.6737e-05, -3.8447e-03,\n",
            "         -2.7143e-02,  3.7836e-06,  2.3411e-40, -2.8915e-02, -2.7452e-02,\n",
            "         -2.7590e-02,  1.2440e-05, -1.9575e-06,  2.6878e-05, -1.2769e-40,\n",
            "         -2.8680e-02, -2.5027e-02, -2.6385e-02,  4.6170e-41,  6.3087e-10],\n",
            "        [-2.3042e-01, -1.3120e-29,  7.4413e-03,  4.0795e-02, -1.4805e-01,\n",
            "          3.8989e-02,  1.3728e-02, -2.3419e-03, -1.8450e-01,  2.1110e-01,\n",
            "         -7.0725e-02, -5.0013e-02, -1.3785e-01, -8.3355e-02, -3.3324e-03,\n",
            "          1.3589e-01,  2.7881e-01, -6.1671e-41, -8.3769e-18,  5.1672e-06],\n",
            "        [ 2.5351e-01,  7.8766e-07,  4.8134e-02,  1.6188e-01,  1.5352e-01,\n",
            "         -5.3465e-02,  1.8966e-01,  3.9569e-03,  2.1650e-03,  1.4212e-01,\n",
            "         -5.4656e-02,  3.8577e-02, -6.0902e-02,  2.2192e-01, -3.1879e-03,\n",
            "          1.5724e-01,  1.3549e-01,  5.4806e-02,  1.0064e-02,  1.3974e-04],\n",
            "        [ 1.2759e-01,  3.6078e-17, -1.7635e-01, -3.3703e-02, -1.8394e-01,\n",
            "          1.1369e-01,  4.9309e-02,  2.7359e-05,  7.5191e-02,  4.1888e-01,\n",
            "          1.1408e-01,  1.0275e-01,  8.1658e-02,  1.9815e-02, -7.4720e-10,\n",
            "         -1.1634e-01, -1.2469e-01, -4.3961e-02, -1.2718e-03, -6.7702e-07],\n",
            "        [-6.6315e-08, -5.7934e-40, -7.0911e-09,  1.9886e-17,  1.9157e-18,\n",
            "          5.5116e-15, -6.6320e-18,  5.2369e-41, -1.6726e-09, -1.1493e-13,\n",
            "         -6.8550e-11, -6.8486e-20,  1.4634e-41, -3.5573e-17,  7.8809e-42,\n",
            "         -1.5180e-08, -4.1403e-11,  1.2605e-37,  5.9953e-41,  3.7077e-31],\n",
            "        [ 2.6727e-02,  1.2471e-09,  8.3451e-02, -1.5845e-01, -1.1015e-02,\n",
            "         -1.3812e-01, -9.7508e-02,  3.1794e-04,  2.5820e-01, -1.0582e-01,\n",
            "          3.0104e-02,  3.2107e-02,  7.0093e-02,  2.6227e-01,  1.4190e-04,\n",
            "         -9.3092e-02,  1.0145e-01, -2.2067e-02, -5.4662e-04,  5.4023e-05],\n",
            "        [-7.5341e-03, -1.8494e-31, -7.8261e-03, -1.3516e-04, -1.5762e-12,\n",
            "         -8.9169e-04, -1.9230e-07, -4.3066e-41, -2.1121e-03, -1.5674e-03,\n",
            "         -4.4624e-03, -1.7056e-20, -6.3436e-20, -4.0795e-06,  4.0605e-41,\n",
            "         -4.1926e-03, -6.7663e-04, -3.7465e-04, -9.6095e-41,  8.7446e-23],\n",
            "        [-1.4948e-02,  1.4823e-13,  7.2484e-02,  6.4514e-02,  1.5092e-01,\n",
            "         -1.0770e-02,  1.3899e-01, -9.3290e-04,  1.5462e-01,  1.6298e-01,\n",
            "          2.2442e-01,  2.8615e-01, -4.9090e-03,  1.4897e-01,  4.2533e-03,\n",
            "          1.3169e-01, -1.3556e-01, -2.0412e-01, -3.3894e-03,  1.5939e-04],\n",
            "        [-1.5710e-01,  6.0280e-33, -1.0598e-02, -9.2985e-02,  3.7322e-02,\n",
            "          4.8381e-02,  2.5972e-02,  2.4044e-07,  1.1346e-01,  1.6126e-01,\n",
            "          3.1670e-01, -2.4759e-02,  5.7881e-02, -9.8592e-02,  9.0326e-10,\n",
            "          1.0976e-03, -9.6868e-02, -4.5097e-02,  3.0556e-04, -1.2366e-07],\n",
            "        [ 3.6328e-02, -4.8357e-24, -7.6088e-02, -1.8813e-01, -2.1552e-01,\n",
            "         -4.9885e-02,  1.1671e-01, -3.0422e-08,  1.8006e-01,  1.5497e-01,\n",
            "          6.4522e-02, -1.8997e-02,  1.2342e-02, -1.7684e-01,  3.5731e-10,\n",
            "         -6.1454e-02, -1.1909e-01, -9.3370e-03,  5.4339e-07, -3.2695e-08],\n",
            "        [ 2.0882e-01, -7.8473e-43, -3.3732e-02, -1.7584e-02,  1.6212e-02,\n",
            "         -1.6590e-01,  1.2193e-01,  9.7053e-05,  1.9394e-01,  1.4436e-02,\n",
            "         -9.3296e-02, -8.1416e-02, -3.9223e-03, -2.1254e-01, -5.2089e-09,\n",
            "          1.9183e-02, -1.8584e-01, -4.9290e-03, -1.7879e-03, -8.5866e-07],\n",
            "        [-2.0832e-03, -8.4135e-36, -1.7139e-02,  1.9215e-01,  1.3072e-02,\n",
            "          6.5222e-03,  8.8871e-02, -3.5094e-19,  3.1259e-02, -4.6048e-02,\n",
            "         -3.4228e-02, -1.0250e-02, -1.4097e-02, -1.1256e-01, -8.6479e-14,\n",
            "         -1.8517e-02, -6.4994e-02, -6.9580e-04,  2.3399e-21, -1.1519e-16],\n",
            "        [-5.1220e-03,  1.4631e-31, -4.3628e-03,  7.1438e-42, -1.3587e-19,\n",
            "         -4.6945e-04, -5.9429e-42, -6.7185e-41, -2.1063e-03, -7.4267e-04,\n",
            "         -1.8967e-03,  4.8653e-42, -2.1616e-41,  5.1491e-39, -1.6322e-40,\n",
            "         -3.3533e-03, -8.1995e-06, -2.0548e-04, -6.6130e-41,  5.7870e-40],\n",
            "        [-7.1460e-05,  1.2452e-40, -2.3491e-05, -1.5726e-10, -7.3771e-17,\n",
            "         -3.2730e-08, -1.1855e-22,  3.7443e-41, -2.0763e-07, -4.4700e-09,\n",
            "         -2.4988e-07, -3.8788e-26,  1.9786e-24, -9.3979e-23,  1.5134e-43,\n",
            "         -1.7934e-05, -3.9950e-10, -2.8687e-09, -4.2171e-41, -3.2901e-41],\n",
            "        [-8.7568e-07, -1.2498e-40, -1.3586e-07, -6.1432e-09,  2.6839e-30,\n",
            "          1.4044e-24,  1.3030e-23, -4.5811e-41, -2.5335e-11,  1.9997e-19,\n",
            "         -1.1189e-12,  2.2258e-41, -5.0080e-33, -5.1395e-14,  3.2314e-42,\n",
            "         -1.3314e-09, -1.7405e-07,  2.6784e-32, -1.2443e-40,  3.3227e-34],\n",
            "        [-3.7478e-02, -1.5644e-35,  1.4732e-01, -5.4599e-02,  6.0122e-02,\n",
            "         -1.8040e-01,  1.6006e-01, -1.7820e-06,  1.0179e-01,  3.9669e-02,\n",
            "          5.1806e-02, -3.9238e-02, -1.8030e-01,  3.4094e-01, -3.0919e-04,\n",
            "         -2.6042e-01, -1.0646e-01, -1.4045e-01,  7.2557e-21,  3.4981e-15],\n",
            "        [-9.0587e-02, -4.7013e-09,  9.3978e-02,  8.6940e-02,  1.2610e-01,\n",
            "          1.2522e-01,  1.1063e-01, -1.0705e-04, -9.4149e-02,  3.4126e-01,\n",
            "          1.0460e-01,  2.7096e-02,  1.2169e-02,  1.6547e-02, -2.1229e-03,\n",
            "          4.5632e-02,  1.1229e-01,  1.3353e-01,  1.6027e-03, -9.9745e-05],\n",
            "        [-1.2153e-01, -9.6611e-38, -8.0763e-03, -8.6743e-03, -8.5965e-02,\n",
            "          1.4151e-01,  6.6330e-02,  1.1750e-08,  1.1603e-01,  2.2451e-01,\n",
            "         -1.1685e-02,  4.4973e-02,  9.4741e-03, -1.1929e-01,  4.6544e-11,\n",
            "          1.0754e-01, -1.1075e-01, -1.0226e-01, -4.1118e-09, -7.5455e-09],\n",
            "        [-4.5074e-02, -2.6272e-12,  4.5109e-02, -1.8342e-01,  9.1591e-02,\n",
            "          1.3882e-01,  1.1175e-01, -2.3252e-04,  8.3935e-02,  2.0617e-02,\n",
            "         -5.6256e-02, -1.6889e-03,  3.0405e-02, -1.9539e-01, -1.7228e-04,\n",
            "         -1.0783e-02,  2.7337e-01,  5.4235e-02,  3.4231e-04, -4.8153e-05],\n",
            "        [-2.0827e-02, -2.9496e-05,  2.5634e-01,  6.1061e-02,  3.2405e-01,\n",
            "          9.6059e-02,  1.1239e-01, -5.1367e-03, -7.6419e-02,  1.8109e-01,\n",
            "         -2.5082e-01, -2.2785e-02,  5.0760e-02,  2.2468e-01,  1.0130e-02,\n",
            "          7.8692e-02,  9.8425e-02,  1.4386e-01,  1.0462e-02, -5.7583e-04],\n",
            "        [ 4.3058e-02, -3.0988e-41, -5.6999e-02, -4.7450e-02, -3.7244e-02,\n",
            "          2.7367e-02,  5.5584e-02, -6.1904e-41,  3.2658e-02, -8.9408e-02,\n",
            "          2.4874e-02,  7.4092e-02, -8.2270e-07, -2.6317e-02,  1.1102e-40,\n",
            "         -2.8635e-02, -2.9394e-02, -2.4038e-02,  6.7557e-42,  8.5409e-41],\n",
            "        [ 4.5211e-02, -2.9063e-36,  2.3448e-03,  1.7151e-01, -1.6830e-01,\n",
            "         -4.8254e-02,  1.9163e-01,  1.0164e-04,  4.5366e-02, -1.0796e-01,\n",
            "         -1.8890e-02,  3.6893e-02,  2.0407e-01, -1.3027e-01,  5.2540e-05,\n",
            "          3.3997e-01,  1.4650e-01,  1.0934e-02, -1.6437e-04,  1.9656e-05],\n",
            "        [-6.7583e-03,  1.5609e-29, -5.6267e-03,  6.5159e-09,  6.5252e-09,\n",
            "         -6.8427e-05, -1.1423e-08, -4.1522e-41, -9.1554e-04, -1.4684e-04,\n",
            "         -1.3997e-03,  6.0241e-09,  3.5559e-09, -2.4930e-09, -1.1686e-40,\n",
            "         -2.5495e-03, -1.9179e-06, -3.3065e-06,  3.4342e-41,  3.3563e-40],\n",
            "        [ 2.2793e-01, -4.3004e-33, -7.6072e-02, -1.0410e-01,  1.5169e-01,\n",
            "         -4.7782e-02, -9.4995e-02,  2.1947e-08, -5.8742e-01, -1.1073e-01,\n",
            "          5.9350e-02, -2.4204e-02, -7.1136e-02, -3.3732e-02, -2.7002e-11,\n",
            "          3.4201e-01,  1.6863e-02,  3.7296e-02,  1.7925e-08,  4.5159e-09],\n",
            "        [-2.6089e-01, -2.4478e-32, -4.5733e-03, -1.0193e-01, -1.4398e-02,\n",
            "          5.9924e-02,  5.6702e-02,  1.3891e-03,  3.6365e-02, -9.2170e-02,\n",
            "         -3.5113e-01,  7.1289e-02,  1.2096e-01,  1.4035e-01, -2.3241e-03,\n",
            "          1.3835e-01,  6.1963e-02, -7.7786e-42, -6.7594e-19, -1.5462e-05],\n",
            "        [-6.8590e-02,  3.0795e-41,  2.4981e-03, -7.2994e-02,  8.0416e-04,\n",
            "         -2.0175e-02,  2.0729e-02,  2.2925e-37,  4.3999e-02,  5.4902e-02,\n",
            "          3.0941e-02, -2.0326e-02, -2.3818e-02, -1.4216e-02, -5.6472e-41,\n",
            "         -8.6029e-02, -3.6591e-02, -5.8859e-02,  7.9124e-36,  4.0153e-41],\n",
            "        [ 1.8852e-01, -2.6583e-42, -2.4858e-01, -7.9679e-03, -1.2091e-01,\n",
            "          1.6607e-01,  4.7848e-02,  6.1189e-07, -2.3200e-02,  2.6224e-01,\n",
            "         -1.6076e-03, -3.6387e-03, -2.0390e-02, -3.3323e-02,  3.8764e-10,\n",
            "          2.6976e-02, -1.7599e-02,  2.5284e-01,  1.4079e-04,  4.9416e-07],\n",
            "        [-2.6159e-03,  1.9013e-35, -1.2694e-09, -9.6124e-03,  2.1030e-01,\n",
            "         -3.6359e-03, -1.5501e-01,  1.5320e-05, -1.3064e-01, -1.8177e-01,\n",
            "         -1.2859e-01,  2.2685e-03, -4.6384e-02,  1.2286e-01,  5.7540e-04,\n",
            "          5.8470e-02,  1.3831e-01, -8.3041e-42,  1.9572e-21, -1.6228e-05],\n",
            "        [-9.4701e-02, -5.1193e-11,  6.5552e-02, -1.6092e-02,  1.4426e-01,\n",
            "         -1.4763e-02,  1.7833e-01, -9.2318e-05,  4.4617e-02,  3.4579e-01,\n",
            "          3.1919e-02,  1.0732e-01,  1.3911e-02,  5.4828e-02,  5.3134e-04,\n",
            "         -8.0084e-03, -3.9007e-02, -2.6720e-02,  5.7637e-04, -5.8770e-05],\n",
            "        [-1.7085e-02,  3.3412e-29, -1.5586e-02,  1.0532e-10,  1.3959e-09,\n",
            "         -3.1312e-03, -9.7577e-10, -1.4750e-40, -8.7077e-03, -4.6798e-03,\n",
            "         -9.3360e-03,  1.0902e-09,  9.0953e-11,  4.5018e-10, -8.1724e-42,\n",
            "         -1.2343e-02, -2.6223e-04, -2.0689e-03,  2.4142e-40,  2.8554e-41],\n",
            "        [-8.3588e-02,  8.9820e-41, -3.5193e-02, -4.1855e-02,  5.8542e-03,\n",
            "          2.1469e-03, -7.7340e-03, -1.5188e-40, -2.0716e-02, -4.5558e-03,\n",
            "          1.8573e-02, -1.2328e-02,  9.7432e-03, -1.3183e-02,  5.6276e-42,\n",
            "          5.1060e-02,  2.6005e-02, -6.0128e-36, -3.1289e-29,  5.0138e-42],\n",
            "        [ 2.0358e-01,  1.6723e-06, -6.4459e-02, -5.3666e-02,  9.5409e-02,\n",
            "         -3.5258e-02,  1.5430e-02, -3.9608e-04,  1.3151e-01, -1.4690e-01,\n",
            "          9.4850e-02,  6.0548e-02,  2.0868e-01,  1.0046e-01,  5.2948e-06,\n",
            "         -1.1763e-01, -2.1046e-01, -1.0328e-01,  1.1695e-02, -2.6386e-07],\n",
            "        [-1.2315e-01, -2.8766e-41,  3.9403e-02, -9.3715e-02,  7.2894e-02,\n",
            "          5.7663e-02, -5.8385e-02,  2.4678e-40,  8.9202e-03, -5.1694e-02,\n",
            "          5.1516e-02, -5.3437e-02, -6.2042e-02, -6.3439e-03,  1.0752e-15,\n",
            "          1.0477e-01,  1.8306e-02, -7.1272e-02, -1.1686e-22, -1.3053e-17],\n",
            "        [ 3.0311e-02, -8.1799e-11, -1.0411e-01,  6.4449e-02,  1.7062e-02,\n",
            "          1.3781e-01, -7.9477e-02, -2.3016e-05,  1.1909e-02, -6.2968e-02,\n",
            "         -5.1806e-02, -2.8258e-02,  1.3658e-01, -3.5126e-01,  5.4965e-07,\n",
            "          1.6398e-01, -1.8952e-02,  1.1464e-01,  4.8168e-04, -5.9565e-09],\n",
            "        [-7.5546e-02, -1.9627e-40,  9.9105e-03, -3.4070e-02,  3.5275e-02,\n",
            "         -3.1123e-02,  2.0022e-02, -4.0869e-41,  4.9989e-02, -1.8601e-01,\n",
            "          4.5610e-03,  1.6400e-02,  2.6773e-19, -6.8613e-02,  9.9294e-40,\n",
            "          6.2082e-03, -3.3847e-02,  1.2895e-41,  1.3082e-40,  6.4205e-40],\n",
            "        [ 1.8296e-02, -8.1152e-32,  1.6798e-01, -6.6395e-02,  7.1667e-02,\n",
            "          1.9277e-02, -6.1527e-03, -4.1625e-07,  5.4279e-02,  1.3128e-01,\n",
            "          4.9822e-02, -3.5137e-02,  7.1291e-02, -1.9606e-02, -9.0206e-10,\n",
            "         -6.8121e-03, -9.3805e-02, -5.5193e-02,  9.0818e-06, -2.3458e-07]],\n",
            "       device='cuda:0'))\n",
            "('2.bias', tensor([ 3.6824e-05,  5.7975e-07,  4.2110e-05,  8.1127e-06, -1.2712e-05,\n",
            "         1.6374e-04, -3.3890e-05,  2.3799e-05,  3.0995e-11, -8.0181e-06,\n",
            "        -9.0292e-06,  5.5039e-05, -3.4810e-05, -1.8989e-04,  1.2045e-04,\n",
            "        -1.0297e-04,  1.7758e-06,  1.5302e-14, -2.4090e-08, -3.9118e-06,\n",
            "        -2.2013e-05, -1.0977e-04, -3.1041e-05,  7.7626e-05,  4.4437e-04,\n",
            "        -8.0624e-06, -6.7769e-08, -6.2569e-05, -1.2867e-05,  8.7080e-05,\n",
            "        -3.8902e-05, -1.1963e-04,  3.0988e-05,  6.2481e-08,  2.5780e-04,\n",
            "        -3.7509e-05, -2.9320e-04, -3.1699e-05, -6.3081e-04, -9.7254e-06],\n",
            "       device='cuda:0'))\n",
            "('4.weight', tensor([[ 7.3055e-02, -5.9910e-09, -4.9841e-02,  1.2769e-04,  2.6225e-02,\n",
            "          6.1962e-02, -2.8667e-01,  1.2811e-01,  2.3282e-07, -6.0623e-02,\n",
            "         -2.6165e-03,  1.9519e-01, -5.9220e-02, -5.9908e-02,  2.2741e-01,\n",
            "          1.8572e-04,  2.6491e-03,  9.1876e-14, -1.1713e-08,  2.0452e-01,\n",
            "          6.5128e-02, -4.1207e-02,  6.3849e-02,  1.4102e-01,  5.3706e-02,\n",
            "          8.6891e-02, -4.4923e-08,  5.1696e-02, -6.5466e-02, -1.8756e-02,\n",
            "         -8.3667e-02, -1.4907e-02, -1.6511e-01, -1.4559e-02,  1.6705e-02,\n",
            "         -8.4293e-02,  4.8722e-02, -2.3526e-01, -1.0920e-01, -3.0801e-02],\n",
            "        [ 1.8838e-01, -1.0406e-08, -1.0363e-02, -3.3204e-05,  2.3586e-02,\n",
            "          1.2745e-02, -1.9176e-01,  2.3834e-02, -5.6722e-19, -8.7162e-02,\n",
            "          4.2602e-03,  1.1287e-01, -3.5241e-02, -9.8154e-02,  1.5648e-01,\n",
            "          4.4971e-02,  6.7555e-07,  7.4195e-15, -3.2367e-15,  1.7451e-01,\n",
            "          3.2391e-02, -2.5480e-02, -4.9921e-03, -8.3331e-02, -2.3695e-02,\n",
            "          3.5561e-02, -1.0181e-08,  4.8147e-02,  3.0316e-02,  1.3472e-02,\n",
            "         -4.3835e-02,  9.0397e-02,  1.6446e-01, -1.6726e-02,  9.6607e-03,\n",
            "         -1.9748e-02,  5.7926e-02, -2.2200e-01, -2.2782e-02,  2.4050e-02],\n",
            "        [ 9.0001e-02,  1.4134e-09,  2.2532e-01, -1.7024e-03,  2.7222e-02,\n",
            "          2.5959e-01, -4.1188e-01,  1.0834e-01,  1.2852e-06,  1.3957e-01,\n",
            "         -6.1678e-03,  1.5902e-01, -6.5155e-02,  3.6791e-02,  1.0196e-01,\n",
            "         -2.0966e-03,  4.8519e-03, -3.1454e-11, -1.5843e-06, -3.6692e-02,\n",
            "         -2.1165e-01,  2.5714e-02, -1.3130e-01,  4.5487e-01,  8.2340e-02,\n",
            "          4.7045e-02,  1.9927e-08, -3.3212e-02, -2.2417e-01, -5.1891e-02,\n",
            "         -3.1512e-02, -1.3349e-01, -5.6751e-02, -4.5242e-03,  4.1242e-02,\n",
            "         -3.4362e-01, -4.6423e-02,  7.6943e-02, -8.2967e-02, -4.8773e-02]],\n",
            "       device='cuda:0'))\n",
            "('4.bias', tensor([-1.0940e-05, -2.2081e-05,  8.3069e-05], device='cuda:0'))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO66CABFM2yK",
        "colab_type": "code",
        "outputId": "b63d15a6-059c-43e2-85f5-1b669889556e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(y_train[1])\n",
        "print(y_train[5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.9865, 0.0135, 0.0000])\n",
            "tensor([0.9796, 0.0204, 0.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJjO9vYqGyVf",
        "colab_type": "code",
        "outputId": "02aa4cbc-5595-4846-8efb-d28b3de526e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "torch.cuda.is_available()\n",
        "USE_GPU = True\n",
        "out = []\n",
        "\n",
        "for i in range(len(Is)):\n",
        "  inp = Is[i]\n",
        "\n",
        "  if USE_GPU == True:\n",
        "      inp = torch.Tensor(inp)\n",
        "      inp = inp.cuda()\n",
        "  ou = model(inp)\n",
        "  ou = ou.cpu()\n",
        "\n",
        "  ou = ou.detach().numpy()\n",
        "  out.append(ou)\n",
        "  \n",
        "  \n",
        "print(out)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([-0.14860675, -0.08928791, -0.08534469], dtype=float32), array([-0.16406867, -0.09797922, -0.12290158], dtype=float32), array([-0.15725562, -0.0877625 , -0.13655628], dtype=float32), array([-0.14866444, -0.08079019, -0.1335782 ], dtype=float32), array([-0.13695428, -0.08361974, -0.10590448], dtype=float32), array([-0.13051555, -0.08563772, -0.08007278], dtype=float32), array([-0.11989447, -0.07983986, -0.07383361], dtype=float32), array([-0.11087701, -0.07249942, -0.06177701], dtype=float32), array([-0.10241696, -0.06385866, -0.05821027], dtype=float32), array([-0.08852288, -0.05377832, -0.06612005], dtype=float32), array([-0.0794749 , -0.04859987, -0.05371131], dtype=float32), array([-0.06840178, -0.04122915, -0.03577973], dtype=float32), array([-0.06439349, -0.03848996, -0.05014171], dtype=float32), array([-0.05508706, -0.02819243, -0.05629184], dtype=float32), array([-0.02963731, -0.01093045, -0.05524554], dtype=float32), array([-0.02713153, -0.01069417, -0.04783346], dtype=float32), array([-0.01194355, -0.01031914, -0.00618984], dtype=float32), array([-0.00703451, -0.00700775, -0.01479878], dtype=float32), array([-0.02035347, -0.00343027, -0.01830681], dtype=float32), array([-0.02059356, -0.00029862, -0.02484788], dtype=float32), array([ 0.00836178,  0.00934971, -0.00593749], dtype=float32), array([ 0.00879498, -0.00123213,  0.0185522 ], dtype=float32), array([-0.01594082, -0.02027981,  0.0188149 ], dtype=float32), array([ 0.00629942, -0.0007655 ,  0.01614049], dtype=float32), array([0.00502941, 0.00054792, 0.01893644], dtype=float32), array([ 0.00238247, -0.00128147,  0.0193852 ], dtype=float32), array([0.00131843, 0.00170577, 0.02244673], dtype=float32), array([0.00126675, 0.00211524, 0.01621864], dtype=float32), array([ 0.00208884, -0.00246566,  0.02410976], dtype=float32), array([ 0.00652633, -0.00018707,  0.02544503], dtype=float32), array([ 0.00038752, -0.00205293,  0.02426646], dtype=float32), array([0.00354414, 0.00011251, 0.02318041], dtype=float32), array([ 0.00033472, -0.00133761,  0.02227585], dtype=float32), array([ 0.00114021, -0.00097575,  0.02507113], dtype=float32), array([0.00264313, 0.00040469, 0.02342556], dtype=float32), array([ 0.00014981, -0.00073799,  0.02323673], dtype=float32), array([ 1.5651563e-03, -2.6431833e-05,  2.5938969e-02], dtype=float32), array([0.00158348, 0.00045553, 0.02488146], dtype=float32), array([ 1.1289993e-03, -7.6559343e-05,  2.7427655e-02], dtype=float32), array([0.00185954, 0.00018886, 0.02790032], dtype=float32), array([ 0.00036578, -0.00069073,  0.0289136 ], dtype=float32), array([ 0.00080058, -0.00051805,  0.02942548], dtype=float32), array([ 0.00072428, -0.00073138,  0.03032015], dtype=float32), array([ 0.00012898, -0.00091448,  0.02973865], dtype=float32), array([-0.0004438 , -0.00100669,  0.02999924], dtype=float32), array([ 0.00051744, -0.00036195,  0.03043054], dtype=float32), array([ 0.00034575, -0.00026896,  0.03183423], dtype=float32), array([0.00141202, 0.00017175, 0.03171728], dtype=float32), array([-0.00198107, -0.00289538,  0.03907869], dtype=float32), array([-0.13460776, -0.07249942, -0.09774961], dtype=float32), array([-0.13984248, -0.08034959, -0.10883083], dtype=float32), array([-0.13028046, -0.07327905, -0.10410895], dtype=float32), array([-0.12118623, -0.06950155, -0.09607973], dtype=float32), array([-0.11596057, -0.09288517, -0.08418908], dtype=float32), array([ 0.02702525,  0.07663187, -0.10887732], dtype=float32), array([-0.06703088, -0.04425302, -0.05443658], dtype=float32), array([-0.01385305,  0.04781923, -0.05742325], dtype=float32), array([-0.04988525, -0.03703156, -0.03874542], dtype=float32), array([ 0.02063849, -0.00459113,  0.01192162], dtype=float32), array([-0.02801177, -0.00062051, -0.05084612], dtype=float32), array([-0.13379464, -0.08181933, -0.19421776], dtype=float32), array([-0.01600394, -0.0158396 , -0.00308426], dtype=float32), array([-0.05953253,  0.02394046, -0.01313108], dtype=float32), array([ 0.01476899,  0.01774869, -0.00206873], dtype=float32), array([0.0264785 , 0.02047137, 0.03106979], dtype=float32), array([-0.02345907, -0.0083427 , -0.01082458], dtype=float32), array([ 0.00252224, -0.00155309,  0.0203486 ], dtype=float32), array([ 0.00238797, -0.00142769,  0.02057684], dtype=float32), array([ 0.00181812, -0.00160636,  0.02154781], dtype=float32), array([ 0.00179541, -0.0012431 ,  0.02000624], dtype=float32), array([ 0.00149867, -0.00180608,  0.02187261], dtype=float32), array([ 0.00162671, -0.00125313,  0.0216958 ], dtype=float32), array([0.0016924 , 0.00042464, 0.01679615], dtype=float32), array([0.00202565, 0.00134325, 0.01902245], dtype=float32), array([ 0.00264624, -0.00042173,  0.02356051], dtype=float32), array([ 0.00248028, -0.00044784,  0.02515797], dtype=float32), array([-0.03304384, -0.00510025,  0.00265161], dtype=float32), array([0.00067076, 0.02910455, 0.00768499], dtype=float32), array([ 0.00090904, -0.00172746,  0.02372302], dtype=float32), array([ 0.00118876, -0.00103607,  0.02307115], dtype=float32), array([ 0.00249953, -0.00054934,  0.02465145], dtype=float32), array([ 0.00209676, -0.00048926,  0.02545736], dtype=float32), array([ 1.7634907e-03, -9.5096388e-05,  2.4436753e-02], dtype=float32), array([2.8777474e-03, 6.4346037e-05, 2.7129751e-02], dtype=float32), array([0.00308952, 0.00045756, 0.02752123], dtype=float32), array([1.5627125e-03, 8.1273756e-05, 2.8004389e-02], dtype=float32), array([0.0010838 , 0.00043801, 0.02688716], dtype=float32), array([0.00103916, 0.00023005, 0.02856527], dtype=float32), array([6.9566986e-06, 1.7771407e-04, 2.8308492e-02], dtype=float32), array([0.00017045, 0.00020078, 0.0285183 ], dtype=float32), array([0.00073127, 0.00011078, 0.03090809], dtype=float32), array([0.00082265, 0.00020287, 0.03096316], dtype=float32), array([ 7.0495247e-05, -1.9755677e-04,  3.2229882e-02], dtype=float32), array([1.72910935e-04, 1.23707905e-05, 3.21922116e-02], dtype=float32), array([ 0.00023967, -0.00017217,  0.03337298], dtype=float32), array([1.5222856e-03, 8.8426314e-05, 3.4909945e-02], dtype=float32), array([ 0.00106981, -0.00016108,  0.03610573], dtype=float32), array([-0.00092955, -0.00211373,  0.04260037], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNwu05vhHRF7",
        "colab_type": "code",
        "outputId": "73925918-1f99-4583-d19c-1067d28f7271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "Lab=[]\n",
        "count=0\n",
        "for i in range(len(Xb)):\n",
        "   temp=[]\n",
        "   for j in range(49):\n",
        "       if USE_GPU == True:\n",
        "        temp.append(out[count])\n",
        "        count+=1\n",
        "   Lab.append(temp)\n",
        "print(Lab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[array([-0.14860675, -0.08928791, -0.08534469], dtype=float32), array([-0.16406867, -0.09797922, -0.12290158], dtype=float32), array([-0.15725562, -0.0877625 , -0.13655628], dtype=float32), array([-0.14866444, -0.08079019, -0.1335782 ], dtype=float32), array([-0.13695428, -0.08361974, -0.10590448], dtype=float32), array([-0.13051555, -0.08563772, -0.08007278], dtype=float32), array([-0.11989447, -0.07983986, -0.07383361], dtype=float32), array([-0.11087701, -0.07249942, -0.06177701], dtype=float32), array([-0.10241696, -0.06385866, -0.05821027], dtype=float32), array([-0.08852288, -0.05377832, -0.06612005], dtype=float32), array([-0.0794749 , -0.04859987, -0.05371131], dtype=float32), array([-0.06840178, -0.04122915, -0.03577973], dtype=float32), array([-0.06439349, -0.03848996, -0.05014171], dtype=float32), array([-0.05508706, -0.02819243, -0.05629184], dtype=float32), array([-0.02963731, -0.01093045, -0.05524554], dtype=float32), array([-0.02713153, -0.01069417, -0.04783346], dtype=float32), array([-0.01194355, -0.01031914, -0.00618984], dtype=float32), array([-0.00703451, -0.00700775, -0.01479878], dtype=float32), array([-0.02035347, -0.00343027, -0.01830681], dtype=float32), array([-0.02059356, -0.00029862, -0.02484788], dtype=float32), array([ 0.00836178,  0.00934971, -0.00593749], dtype=float32), array([ 0.00879498, -0.00123213,  0.0185522 ], dtype=float32), array([-0.01594082, -0.02027981,  0.0188149 ], dtype=float32), array([ 0.00629942, -0.0007655 ,  0.01614049], dtype=float32), array([0.00502941, 0.00054792, 0.01893644], dtype=float32), array([ 0.00238247, -0.00128147,  0.0193852 ], dtype=float32), array([0.00131843, 0.00170577, 0.02244673], dtype=float32), array([0.00126675, 0.00211524, 0.01621864], dtype=float32), array([ 0.00208884, -0.00246566,  0.02410976], dtype=float32), array([ 0.00652633, -0.00018707,  0.02544503], dtype=float32), array([ 0.00038752, -0.00205293,  0.02426646], dtype=float32), array([0.00354414, 0.00011251, 0.02318041], dtype=float32), array([ 0.00033472, -0.00133761,  0.02227585], dtype=float32), array([ 0.00114021, -0.00097575,  0.02507113], dtype=float32), array([0.00264313, 0.00040469, 0.02342556], dtype=float32), array([ 0.00014981, -0.00073799,  0.02323673], dtype=float32), array([ 1.5651563e-03, -2.6431833e-05,  2.5938969e-02], dtype=float32), array([0.00158348, 0.00045553, 0.02488146], dtype=float32), array([ 1.1289993e-03, -7.6559343e-05,  2.7427655e-02], dtype=float32), array([0.00185954, 0.00018886, 0.02790032], dtype=float32), array([ 0.00036578, -0.00069073,  0.0289136 ], dtype=float32), array([ 0.00080058, -0.00051805,  0.02942548], dtype=float32), array([ 0.00072428, -0.00073138,  0.03032015], dtype=float32), array([ 0.00012898, -0.00091448,  0.02973865], dtype=float32), array([-0.0004438 , -0.00100669,  0.02999924], dtype=float32), array([ 0.00051744, -0.00036195,  0.03043054], dtype=float32), array([ 0.00034575, -0.00026896,  0.03183423], dtype=float32), array([0.00141202, 0.00017175, 0.03171728], dtype=float32), array([-0.00198107, -0.00289538,  0.03907869], dtype=float32)], [array([-0.13460776, -0.07249942, -0.09774961], dtype=float32), array([-0.13984248, -0.08034959, -0.10883083], dtype=float32), array([-0.13028046, -0.07327905, -0.10410895], dtype=float32), array([-0.12118623, -0.06950155, -0.09607973], dtype=float32), array([-0.11596057, -0.09288517, -0.08418908], dtype=float32), array([ 0.02702525,  0.07663187, -0.10887732], dtype=float32), array([-0.06703088, -0.04425302, -0.05443658], dtype=float32), array([-0.01385305,  0.04781923, -0.05742325], dtype=float32), array([-0.04988525, -0.03703156, -0.03874542], dtype=float32), array([ 0.02063849, -0.00459113,  0.01192162], dtype=float32), array([-0.02801177, -0.00062051, -0.05084612], dtype=float32), array([-0.13379464, -0.08181933, -0.19421776], dtype=float32), array([-0.01600394, -0.0158396 , -0.00308426], dtype=float32), array([-0.05953253,  0.02394046, -0.01313108], dtype=float32), array([ 0.01476899,  0.01774869, -0.00206873], dtype=float32), array([0.0264785 , 0.02047137, 0.03106979], dtype=float32), array([-0.02345907, -0.0083427 , -0.01082458], dtype=float32), array([ 0.00252224, -0.00155309,  0.0203486 ], dtype=float32), array([ 0.00238797, -0.00142769,  0.02057684], dtype=float32), array([ 0.00181812, -0.00160636,  0.02154781], dtype=float32), array([ 0.00179541, -0.0012431 ,  0.02000624], dtype=float32), array([ 0.00149867, -0.00180608,  0.02187261], dtype=float32), array([ 0.00162671, -0.00125313,  0.0216958 ], dtype=float32), array([0.0016924 , 0.00042464, 0.01679615], dtype=float32), array([0.00202565, 0.00134325, 0.01902245], dtype=float32), array([ 0.00264624, -0.00042173,  0.02356051], dtype=float32), array([ 0.00248028, -0.00044784,  0.02515797], dtype=float32), array([-0.03304384, -0.00510025,  0.00265161], dtype=float32), array([0.00067076, 0.02910455, 0.00768499], dtype=float32), array([ 0.00090904, -0.00172746,  0.02372302], dtype=float32), array([ 0.00118876, -0.00103607,  0.02307115], dtype=float32), array([ 0.00249953, -0.00054934,  0.02465145], dtype=float32), array([ 0.00209676, -0.00048926,  0.02545736], dtype=float32), array([ 1.7634907e-03, -9.5096388e-05,  2.4436753e-02], dtype=float32), array([2.8777474e-03, 6.4346037e-05, 2.7129751e-02], dtype=float32), array([0.00308952, 0.00045756, 0.02752123], dtype=float32), array([1.5627125e-03, 8.1273756e-05, 2.8004389e-02], dtype=float32), array([0.0010838 , 0.00043801, 0.02688716], dtype=float32), array([0.00103916, 0.00023005, 0.02856527], dtype=float32), array([6.9566986e-06, 1.7771407e-04, 2.8308492e-02], dtype=float32), array([0.00017045, 0.00020078, 0.0285183 ], dtype=float32), array([0.00073127, 0.00011078, 0.03090809], dtype=float32), array([0.00082265, 0.00020287, 0.03096316], dtype=float32), array([ 7.0495247e-05, -1.9755677e-04,  3.2229882e-02], dtype=float32), array([1.72910935e-04, 1.23707905e-05, 3.21922116e-02], dtype=float32), array([ 0.00023967, -0.00017217,  0.03337298], dtype=float32), array([1.5222856e-03, 8.8426314e-05, 3.4909945e-02], dtype=float32), array([ 0.00106981, -0.00016108,  0.03610573], dtype=float32), array([-0.00092955, -0.00211373,  0.04260037], dtype=float32)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-qv93wSHW-x",
        "colab_type": "code",
        "outputId": "f5e821d2-624d-4607-8b1a-fa2be6bf8592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "file = \"Store_NN_test\"\n",
        "with open(file, \"wb\") as f:\n",
        "    pk.dump([Xname,Xb, Xs,Lab], f)\n",
        "print('NN done')\n",
        "\n",
        "file = 'bestloss'\n",
        "with open(file,'wb') as f:\n",
        "  pk.dump([TRAIN_LOSS, VAL_LOSS], f)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl1KSS75G3zr",
        "colab_type": "text"
      },
      "source": [
        "#end Yao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM4NlVvIG3yH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM-HfJB46Nlu",
        "colab_type": "code",
        "outputId": "9b03575c-e10c-496c-cd93-f25ade6c6480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "check_accuracy(val_loader,model)\n",
        "check_accuracy(test_loader,model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation or test loss is tensor(0.0214, device='cuda:0')\n",
            "validation or test loss is tensor(0.0212, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0212, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzt5cHW93sbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in model.state_dict().items():\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMmGQ0TecVWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lossf = nn.MSELoss(reduction='mean')\n",
        "# asp = []\n",
        "# ap = []\n",
        "# asu = []\n",
        "out = []\n",
        "tars = []\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  inp = X_test[i]\n",
        "  tar = y_test[i]\n",
        "#   ind = np.random.randint(0,len(X_test)-1,1)\n",
        "#   tein = X_test[ind,:]\n",
        "#   tetar = y_test[ind,:]\n",
        "  if USE_GPU == True:\n",
        "      inp = inp.cuda()\n",
        "      tar = tar.cuda()\n",
        "#   start = time.time()\n",
        "  ou = model(inp)\n",
        "  \n",
        "  loss = lossf(ou,tar.float())\n",
        "#   print(loss)\n",
        "#   if loss > 1:\n",
        "#     print(i)\n",
        "#   end = time.time()\n",
        "#   mtime = end - start\n",
        "#   print('each modeling time is',mtime)\n",
        "#   if USE_GPU == True: #each modeling time is 0.0008425712585449219\n",
        "#       out = out.cpu()\n",
        "#       inp = inp.cpu()\n",
        "#   out = out.detach().numpy()\n",
        "  ou = ou.cpu()\n",
        "  tar = tar.cpu()\n",
        "  ou = ou.detach().numpy()\n",
        "  tar = tar.detach().numpy()\n",
        "  out.append(ou)\n",
        "  tars.append(tar)\n",
        " \n",
        "asp = [out[i][0] - tars[i][0] for i in range(len(out))]\n",
        "ap = [out[i][1] - tars[i][1] for i in range(len(out))]\n",
        "asu = [out[i][2] - tars[i][2] for i in range(len(out))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TjLodQ88XvP",
        "colab_type": "code",
        "outputId": "df57e670-5cfc-4369-b645-71037200b51d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2K7OXfiYFTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.hist(asp,bins = np.arange(-1,1,0.1))\n",
        "\n",
        "# plt.hist(ap,bins = np.arange(-1,1,0.1))\n",
        "plt.hist(asu,bins = np.arange(-1,1,0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfYAM9zGZzAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIAfdgCkBk7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the number of the parameters\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zehG7tHvdlN",
        "colab_type": "text"
      },
      "source": [
        "#Visulization of some prediction errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOFS3Z3xvki3",
        "colab_type": "code",
        "outputId": "d44d45c4-3792-48e4-f36f-dd013a5a6ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lossf = nn.MSELoss(reduction='mean')\n",
        "# asp = []\n",
        "# ap = []\n",
        "# asu = []\n",
        "out = []\n",
        "tars = []\n",
        "\n",
        "for i in range(100):\n",
        "  inp = X_val[i,:]\n",
        "  tar = y_val[i,:]\n",
        "#   ind = np.random.randint(0,len(X_test)-1,1)\n",
        "#   tein = X_test[ind,:]\n",
        "#   tetar = y_test[ind,:]\n",
        "  if USE_GPU == True:\n",
        "      inp = inp.cuda()\n",
        "      tar = tar.cuda()\n",
        "#   start = time.time()\n",
        "  ou = best_model(inp)\n",
        "  \n",
        "  loss = lossf(ou,tar.float())\n",
        "#   print(loss)\n",
        "#   if loss > 1:\n",
        "#     print(i)\n",
        "#   end = time.time()\n",
        "#   mtime = end - start\n",
        "#   print('each modeling time is',mtime)\n",
        "#   if USE_GPU == True: #each modeling time is 0.0008425712585449219\n",
        "#       out = out.cpu()\n",
        "#       inp = inp.cpu()\n",
        "#   out = out.detach().numpy()\n",
        "  ou = ou.cpu()\n",
        "  tar = tar.cpu()\n",
        "  ou = ou.detach().numpy()\n",
        "  tar = tar.detach().numpy()\n",
        "  out.append(ou)\n",
        "  tars.append(tar)\n",
        "\n",
        "asp = [out[i][0] - tars[i][0] for i in range(len(out))]\n",
        "ap = [out[i][1] - tars[i][1] for i in range(len(out))]\n",
        "asu = [out[i][2] - tars[i][2] for i in range(len(out))]\n",
        "plt.hist(asp,bins = np.arange(-1,1,0.01))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  1., 12., 15.,  5.,  3.,  4.,  3.,\n",
              "         2.,  2.,  2.,  1.,  6.,  2.,  1.,  0.,  1.,  0.,  1.,  0.,  2.,\n",
              "         2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  1.,\n",
              "         0.,  1.,  0.,  1.,  0.,  2.,  1.,  0.,  2.,  0.,  0.,  0.,  0.,\n",
              "         1.,  1.,  0.,  0.,  2.,  2.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
              "         1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  1.,  2.,  7.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.]),\n",
              " array([-1.0000000e+00, -9.9000000e-01, -9.8000000e-01, -9.7000000e-01,\n",
              "        -9.6000000e-01, -9.5000000e-01, -9.4000000e-01, -9.3000000e-01,\n",
              "        -9.2000000e-01, -9.1000000e-01, -9.0000000e-01, -8.9000000e-01,\n",
              "        -8.8000000e-01, -8.7000000e-01, -8.6000000e-01, -8.5000000e-01,\n",
              "        -8.4000000e-01, -8.3000000e-01, -8.2000000e-01, -8.1000000e-01,\n",
              "        -8.0000000e-01, -7.9000000e-01, -7.8000000e-01, -7.7000000e-01,\n",
              "        -7.6000000e-01, -7.5000000e-01, -7.4000000e-01, -7.3000000e-01,\n",
              "        -7.2000000e-01, -7.1000000e-01, -7.0000000e-01, -6.9000000e-01,\n",
              "        -6.8000000e-01, -6.7000000e-01, -6.6000000e-01, -6.5000000e-01,\n",
              "        -6.4000000e-01, -6.3000000e-01, -6.2000000e-01, -6.1000000e-01,\n",
              "        -6.0000000e-01, -5.9000000e-01, -5.8000000e-01, -5.7000000e-01,\n",
              "        -5.6000000e-01, -5.5000000e-01, -5.4000000e-01, -5.3000000e-01,\n",
              "        -5.2000000e-01, -5.1000000e-01, -5.0000000e-01, -4.9000000e-01,\n",
              "        -4.8000000e-01, -4.7000000e-01, -4.6000000e-01, -4.5000000e-01,\n",
              "        -4.4000000e-01, -4.3000000e-01, -4.2000000e-01, -4.1000000e-01,\n",
              "        -4.0000000e-01, -3.9000000e-01, -3.8000000e-01, -3.7000000e-01,\n",
              "        -3.6000000e-01, -3.5000000e-01, -3.4000000e-01, -3.3000000e-01,\n",
              "        -3.2000000e-01, -3.1000000e-01, -3.0000000e-01, -2.9000000e-01,\n",
              "        -2.8000000e-01, -2.7000000e-01, -2.6000000e-01, -2.5000000e-01,\n",
              "        -2.4000000e-01, -2.3000000e-01, -2.2000000e-01, -2.1000000e-01,\n",
              "        -2.0000000e-01, -1.9000000e-01, -1.8000000e-01, -1.7000000e-01,\n",
              "        -1.6000000e-01, -1.5000000e-01, -1.4000000e-01, -1.3000000e-01,\n",
              "        -1.2000000e-01, -1.1000000e-01, -1.0000000e-01, -9.0000000e-02,\n",
              "        -8.0000000e-02, -7.0000000e-02, -6.0000000e-02, -5.0000000e-02,\n",
              "        -4.0000000e-02, -3.0000000e-02, -2.0000000e-02, -1.0000000e-02,\n",
              "         8.8817842e-16,  1.0000000e-02,  2.0000000e-02,  3.0000000e-02,\n",
              "         4.0000000e-02,  5.0000000e-02,  6.0000000e-02,  7.0000000e-02,\n",
              "         8.0000000e-02,  9.0000000e-02,  1.0000000e-01,  1.1000000e-01,\n",
              "         1.2000000e-01,  1.3000000e-01,  1.4000000e-01,  1.5000000e-01,\n",
              "         1.6000000e-01,  1.7000000e-01,  1.8000000e-01,  1.9000000e-01,\n",
              "         2.0000000e-01,  2.1000000e-01,  2.2000000e-01,  2.3000000e-01,\n",
              "         2.4000000e-01,  2.5000000e-01,  2.6000000e-01,  2.7000000e-01,\n",
              "         2.8000000e-01,  2.9000000e-01,  3.0000000e-01,  3.1000000e-01,\n",
              "         3.2000000e-01,  3.3000000e-01,  3.4000000e-01,  3.5000000e-01,\n",
              "         3.6000000e-01,  3.7000000e-01,  3.8000000e-01,  3.9000000e-01,\n",
              "         4.0000000e-01,  4.1000000e-01,  4.2000000e-01,  4.3000000e-01,\n",
              "         4.4000000e-01,  4.5000000e-01,  4.6000000e-01,  4.7000000e-01,\n",
              "         4.8000000e-01,  4.9000000e-01,  5.0000000e-01,  5.1000000e-01,\n",
              "         5.2000000e-01,  5.3000000e-01,  5.4000000e-01,  5.5000000e-01,\n",
              "         5.6000000e-01,  5.7000000e-01,  5.8000000e-01,  5.9000000e-01,\n",
              "         6.0000000e-01,  6.1000000e-01,  6.2000000e-01,  6.3000000e-01,\n",
              "         6.4000000e-01,  6.5000000e-01,  6.6000000e-01,  6.7000000e-01,\n",
              "         6.8000000e-01,  6.9000000e-01,  7.0000000e-01,  7.1000000e-01,\n",
              "         7.2000000e-01,  7.3000000e-01,  7.4000000e-01,  7.5000000e-01,\n",
              "         7.6000000e-01,  7.7000000e-01,  7.8000000e-01,  7.9000000e-01,\n",
              "         8.0000000e-01,  8.1000000e-01,  8.2000000e-01,  8.3000000e-01,\n",
              "         8.4000000e-01,  8.5000000e-01,  8.6000000e-01,  8.7000000e-01,\n",
              "         8.8000000e-01,  8.9000000e-01,  9.0000000e-01,  9.1000000e-01,\n",
              "         9.2000000e-01,  9.3000000e-01,  9.4000000e-01,  9.5000000e-01,\n",
              "         9.6000000e-01,  9.7000000e-01,  9.8000000e-01,  9.9000000e-01]),\n",
              " <a list of 199 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFh9Sboc7NV0",
        "colab_type": "text"
      },
      "source": [
        "#Save the model parameters to a pickled file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfPjEXTo7iuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = 'FC'\n",
        "torch.save(model.state_dict(), file_name)\n",
        "\n",
        "# file_name = 'models'\n",
        "# with open(file_name, \"wb\") as f:\n",
        "#     pk.dump(results, f)\n",
        "#state_dict = torch.load('checkpoint.pth')\n",
        "#model.load_state_dict(state_dict)\n",
        "\n",
        "# file_name = 'models'\n",
        "# with open(file_name, \"rb\") as f:\n",
        "# # #     results = pk.load(f)\n",
        "# VAL_LOSS = [VAL_LOSS[i].cpu() for i in range(len(VAL_LOSS))]\n",
        "# VAL_LOSS = [VAL_LOSS[I].detach().numpy() for I in range(len(VAL_LOSS))]\n",
        "# TRAIN_LOSS = [TRAIN_LOSS[i].cpu() for i in range(len(VAL_LOSS))]\n",
        "# TRAIN_LOSS = [TRAIN_LOSS[I].detach().numpy() for I in range(len(TRAIN_LOSS))]\n",
        "\n",
        "file_name = 'bestloss'\n",
        "with open(file_name, \"wb\") as f:\n",
        "    pk.dump([TRAIN_LOSS,VAL_LOSS], f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R6c-UbjyjpR",
        "colab_type": "code",
        "outputId": "8a8e3911-aae0-42ee-8f00-0a72ad66167c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "TRAIN_LOSS[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(2239.277, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IAPXJAUguR7",
        "colab_type": "text"
      },
      "source": [
        "#Do not train if using the model\n",
        ", start from here after data spliting, need to put FC model state dictionary into the current working space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRL9k98VgtvL",
        "colab_type": "code",
        "outputId": "9d39ed94-46d4-4669-968c-848b51361ab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "state_dict = torch.load('FC')\n",
        "hidden1 = 20\n",
        "hidden2 = 80\n",
        "hidden3 = 40\n",
        "inputsize = 11\n",
        "\n",
        "model = nn.Sequential(\n",
        "              nn.Linear(inputsize, hidden1),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden1,hidden2),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden2,hidden3),\n",
        "              nn.ReLU(),\n",
        "          #     nn.Linear(hidden3,hidden4),\n",
        "          #     nn.ReLU(),\n",
        "          #     nn.Linear(hidden4,hidden3),\n",
        "          #     nn.ReLU(),\n",
        "              nn.Linear(hidden3,hidden2),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden2,hidden1),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden1,3),\n",
        "              nn.Softmax()\n",
        "          ).cuda()\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BkJJZ9Soe1G",
        "colab_type": "code",
        "outputId": "c3385b57-1a8e-4b2f-e005-5b0b2f0bdbb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "check_accuracy(test_loader, model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation or test loss is tensor(0.1114, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1114, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXxcGuCghoEu",
        "colab_type": "code",
        "outputId": "7eff0c63-9f97-48df-9b26-d77396df493b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lossf = nn.MSELoss(reduction='mean')\n",
        "# asp = []\n",
        "# ap = []\n",
        "# asu = []\n",
        "out = []\n",
        "tars = []\n",
        "\n",
        "for i in range(100):\n",
        "  inp = X_val[i,:]\n",
        "  tar = y_val[i,:]\n",
        "#   ind = np.random.randint(0,len(X_test)-1,1)\n",
        "#   tein = X_test[ind,:]\n",
        "#   tetar = y_test[ind,:]\n",
        "  if USE_GPU == True:\n",
        "      inp = inp.cuda()\n",
        "      tar = tar.cuda()\n",
        "#   start = time.time()\n",
        "  ou = best_model(inp)\n",
        "  \n",
        "  loss = lossf(ou,tar.float())\n",
        "#   print(loss)\n",
        "#   if loss > 1:\n",
        "#     print(i)\n",
        "#   end = time.time()\n",
        "#   mtime = end - start\n",
        "#   print('each modeling time is',mtime)\n",
        "#   if USE_GPU == True: #each modeling time is 0.0008425712585449219\n",
        "#       out = out.cpu()\n",
        "#       inp = inp.cpu()\n",
        "#   out = out.detach().numpy()\n",
        "  ou = ou.cpu()\n",
        "  tar = tar.cpu()\n",
        "  ou = ou.detach().numpy()\n",
        "  tar = tar.detach().numpy()\n",
        "  out.append(ou)\n",
        "  tars.append(tar)\n",
        "\n",
        "asp = [out[i][0] - tars[i][0] for i in range(len(out))]\n",
        "ap = [out[i][1] - tars[i][1] for i in range(len(out))]\n",
        "asu = [out[i][2] - tars[i][2] for i in range(len(out))]\n",
        "plt.hist(ap,bins = np.arange(-1,1,0.01))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  4.,  0.,  0.,  0.,  0.,\n",
              "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  2.,  0.,  1.,  0.,  0.,\n",
              "         0.,  1.,  0.,  0.,  2.,  1.,  1.,  0.,  0.,  2.,  0.,  0.,  0.,\n",
              "         0.,  2.,  0.,  0.,  1.,  2.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
              "         0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  2.,  1.,\n",
              "         0.,  0.,  1.,  0.,  2.,  6.,  2.,  2.,  0.,  3.,  3.,  2.,  5.,\n",
              "         3.,  9., 17.,  6.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "         0.,  0.,  0.,  0.]),\n",
              " array([-1.0000000e+00, -9.9000000e-01, -9.8000000e-01, -9.7000000e-01,\n",
              "        -9.6000000e-01, -9.5000000e-01, -9.4000000e-01, -9.3000000e-01,\n",
              "        -9.2000000e-01, -9.1000000e-01, -9.0000000e-01, -8.9000000e-01,\n",
              "        -8.8000000e-01, -8.7000000e-01, -8.6000000e-01, -8.5000000e-01,\n",
              "        -8.4000000e-01, -8.3000000e-01, -8.2000000e-01, -8.1000000e-01,\n",
              "        -8.0000000e-01, -7.9000000e-01, -7.8000000e-01, -7.7000000e-01,\n",
              "        -7.6000000e-01, -7.5000000e-01, -7.4000000e-01, -7.3000000e-01,\n",
              "        -7.2000000e-01, -7.1000000e-01, -7.0000000e-01, -6.9000000e-01,\n",
              "        -6.8000000e-01, -6.7000000e-01, -6.6000000e-01, -6.5000000e-01,\n",
              "        -6.4000000e-01, -6.3000000e-01, -6.2000000e-01, -6.1000000e-01,\n",
              "        -6.0000000e-01, -5.9000000e-01, -5.8000000e-01, -5.7000000e-01,\n",
              "        -5.6000000e-01, -5.5000000e-01, -5.4000000e-01, -5.3000000e-01,\n",
              "        -5.2000000e-01, -5.1000000e-01, -5.0000000e-01, -4.9000000e-01,\n",
              "        -4.8000000e-01, -4.7000000e-01, -4.6000000e-01, -4.5000000e-01,\n",
              "        -4.4000000e-01, -4.3000000e-01, -4.2000000e-01, -4.1000000e-01,\n",
              "        -4.0000000e-01, -3.9000000e-01, -3.8000000e-01, -3.7000000e-01,\n",
              "        -3.6000000e-01, -3.5000000e-01, -3.4000000e-01, -3.3000000e-01,\n",
              "        -3.2000000e-01, -3.1000000e-01, -3.0000000e-01, -2.9000000e-01,\n",
              "        -2.8000000e-01, -2.7000000e-01, -2.6000000e-01, -2.5000000e-01,\n",
              "        -2.4000000e-01, -2.3000000e-01, -2.2000000e-01, -2.1000000e-01,\n",
              "        -2.0000000e-01, -1.9000000e-01, -1.8000000e-01, -1.7000000e-01,\n",
              "        -1.6000000e-01, -1.5000000e-01, -1.4000000e-01, -1.3000000e-01,\n",
              "        -1.2000000e-01, -1.1000000e-01, -1.0000000e-01, -9.0000000e-02,\n",
              "        -8.0000000e-02, -7.0000000e-02, -6.0000000e-02, -5.0000000e-02,\n",
              "        -4.0000000e-02, -3.0000000e-02, -2.0000000e-02, -1.0000000e-02,\n",
              "         8.8817842e-16,  1.0000000e-02,  2.0000000e-02,  3.0000000e-02,\n",
              "         4.0000000e-02,  5.0000000e-02,  6.0000000e-02,  7.0000000e-02,\n",
              "         8.0000000e-02,  9.0000000e-02,  1.0000000e-01,  1.1000000e-01,\n",
              "         1.2000000e-01,  1.3000000e-01,  1.4000000e-01,  1.5000000e-01,\n",
              "         1.6000000e-01,  1.7000000e-01,  1.8000000e-01,  1.9000000e-01,\n",
              "         2.0000000e-01,  2.1000000e-01,  2.2000000e-01,  2.3000000e-01,\n",
              "         2.4000000e-01,  2.5000000e-01,  2.6000000e-01,  2.7000000e-01,\n",
              "         2.8000000e-01,  2.9000000e-01,  3.0000000e-01,  3.1000000e-01,\n",
              "         3.2000000e-01,  3.3000000e-01,  3.4000000e-01,  3.5000000e-01,\n",
              "         3.6000000e-01,  3.7000000e-01,  3.8000000e-01,  3.9000000e-01,\n",
              "         4.0000000e-01,  4.1000000e-01,  4.2000000e-01,  4.3000000e-01,\n",
              "         4.4000000e-01,  4.5000000e-01,  4.6000000e-01,  4.7000000e-01,\n",
              "         4.8000000e-01,  4.9000000e-01,  5.0000000e-01,  5.1000000e-01,\n",
              "         5.2000000e-01,  5.3000000e-01,  5.4000000e-01,  5.5000000e-01,\n",
              "         5.6000000e-01,  5.7000000e-01,  5.8000000e-01,  5.9000000e-01,\n",
              "         6.0000000e-01,  6.1000000e-01,  6.2000000e-01,  6.3000000e-01,\n",
              "         6.4000000e-01,  6.5000000e-01,  6.6000000e-01,  6.7000000e-01,\n",
              "         6.8000000e-01,  6.9000000e-01,  7.0000000e-01,  7.1000000e-01,\n",
              "         7.2000000e-01,  7.3000000e-01,  7.4000000e-01,  7.5000000e-01,\n",
              "         7.6000000e-01,  7.7000000e-01,  7.8000000e-01,  7.9000000e-01,\n",
              "         8.0000000e-01,  8.1000000e-01,  8.2000000e-01,  8.3000000e-01,\n",
              "         8.4000000e-01,  8.5000000e-01,  8.6000000e-01,  8.7000000e-01,\n",
              "         8.8000000e-01,  8.9000000e-01,  9.0000000e-01,  9.1000000e-01,\n",
              "         9.2000000e-01,  9.3000000e-01,  9.4000000e-01,  9.5000000e-01,\n",
              "         9.6000000e-01,  9.7000000e-01,  9.8000000e-01,  9.9000000e-01]),\n",
              " <a list of 199 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxP8DltwSyFlrlW8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}